{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "data = pd.read_csv(\"cardiotocographic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>DL</th>\n",
       "      <th>DS</th>\n",
       "      <th>DP</th>\n",
       "      <th>ASTV</th>\n",
       "      <th>MSTV</th>\n",
       "      <th>ALTV</th>\n",
       "      <th>...</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Nmax</th>\n",
       "      <th>Nzeros</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Tendency</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>133.303857</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>46.990122</td>\n",
       "      <td>1.332785</td>\n",
       "      <td>9.84666</td>\n",
       "      <td>...</td>\n",
       "      <td>93.579492</td>\n",
       "      <td>164.025400</td>\n",
       "      <td>4.068203</td>\n",
       "      <td>0.323612</td>\n",
       "      <td>137.452023</td>\n",
       "      <td>134.610536</td>\n",
       "      <td>138.090310</td>\n",
       "      <td>18.808090</td>\n",
       "      <td>0.320320</td>\n",
       "      <td>1.304327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.840844</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.046670</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>17.192814</td>\n",
       "      <td>0.883241</td>\n",
       "      <td>18.39688</td>\n",
       "      <td>...</td>\n",
       "      <td>29.560212</td>\n",
       "      <td>17.944183</td>\n",
       "      <td>2.949386</td>\n",
       "      <td>0.706059</td>\n",
       "      <td>16.381289</td>\n",
       "      <td>15.593596</td>\n",
       "      <td>14.466589</td>\n",
       "      <td>28.977636</td>\n",
       "      <td>0.610829</td>\n",
       "      <td>0.614377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>133.000000</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.019284</td>\n",
       "      <td>0.480634</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>91.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                LB           AC           FM           UC           DL  \\\n",
       "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000   \n",
       "mean    133.303857     0.003170     0.009474     0.004357     0.001885   \n",
       "std       9.840844     0.003860     0.046670     0.002940     0.002962   \n",
       "min     106.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%     126.000000     0.000000     0.000000     0.001876     0.000000   \n",
       "50%     133.000000     0.001630     0.000000     0.004482     0.000000   \n",
       "75%     140.000000     0.005631     0.002512     0.006525     0.003264   \n",
       "max     160.000000     0.019284     0.480634     0.014925     0.015385   \n",
       "\n",
       "                DS           DP         ASTV         MSTV        ALTV  \\\n",
       "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.00000   \n",
       "mean      0.000004     0.000157    46.990122     1.332785     9.84666   \n",
       "std       0.000063     0.000580    17.192814     0.883241    18.39688   \n",
       "min       0.000000     0.000000    12.000000     0.200000     0.00000   \n",
       "25%       0.000000     0.000000    32.000000     0.700000     0.00000   \n",
       "50%       0.000000     0.000000    49.000000     1.200000     0.00000   \n",
       "75%       0.000000     0.000000    61.000000     1.700000    11.00000   \n",
       "max       0.001353     0.005348    87.000000     7.000000    91.00000   \n",
       "\n",
       "          ...               Min          Max         Nmax       Nzeros  \\\n",
       "count     ...       2126.000000  2126.000000  2126.000000  2126.000000   \n",
       "mean      ...         93.579492   164.025400     4.068203     0.323612   \n",
       "std       ...         29.560212    17.944183     2.949386     0.706059   \n",
       "min       ...         50.000000   122.000000     0.000000     0.000000   \n",
       "25%       ...         67.000000   152.000000     2.000000     0.000000   \n",
       "50%       ...         93.000000   162.000000     3.000000     0.000000   \n",
       "75%       ...        120.000000   174.000000     6.000000     0.000000   \n",
       "max       ...        159.000000   238.000000    18.000000    10.000000   \n",
       "\n",
       "              Mode         Mean       Median     Variance     Tendency  \\\n",
       "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000   \n",
       "mean    137.452023   134.610536   138.090310    18.808090     0.320320   \n",
       "std      16.381289    15.593596    14.466589    28.977636     0.610829   \n",
       "min      60.000000    73.000000    77.000000     0.000000    -1.000000   \n",
       "25%     129.000000   125.000000   129.000000     2.000000     0.000000   \n",
       "50%     139.000000   136.000000   139.000000     7.000000     0.000000   \n",
       "75%     148.000000   145.000000   148.000000    24.000000     1.000000   \n",
       "max     187.000000   182.000000   186.000000   269.000000     1.000000   \n",
       "\n",
       "               NSP  \n",
       "count  2126.000000  \n",
       "mean      1.304327  \n",
       "std       0.614377  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       1.000000  \n",
       "max       3.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describing the dataset\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2126 entries, 0 to 2125\n",
      "Data columns (total 22 columns):\n",
      "LB          2126 non-null int64\n",
      "AC          2126 non-null float64\n",
      "FM          2126 non-null float64\n",
      "UC          2126 non-null float64\n",
      "DL          2126 non-null float64\n",
      "DS          2126 non-null float64\n",
      "DP          2126 non-null float64\n",
      "ASTV        2126 non-null int64\n",
      "MSTV        2126 non-null float64\n",
      "ALTV        2126 non-null int64\n",
      "MLTV        2126 non-null float64\n",
      "Width       2126 non-null int64\n",
      "Min         2126 non-null int64\n",
      "Max         2126 non-null int64\n",
      "Nmax        2126 non-null int64\n",
      "Nzeros      2126 non-null int64\n",
      "Mode        2126 non-null int64\n",
      "Mean        2126 non-null int64\n",
      "Median      2126 non-null int64\n",
      "Variance    2126 non-null int64\n",
      "Tendency    2126 non-null int64\n",
      "NSP         2126 non-null int64\n",
      "dtypes: float64(8), int64(14)\n",
      "memory usage: 365.5 KB\n"
     ]
    }
   ],
   "source": [
    "# checking the data structure\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>DL</th>\n",
       "      <th>DS</th>\n",
       "      <th>DP</th>\n",
       "      <th>ASTV</th>\n",
       "      <th>MSTV</th>\n",
       "      <th>ALTV</th>\n",
       "      <th>...</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Nmax</th>\n",
       "      <th>Nzeros</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Tendency</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>137</td>\n",
       "      <td>121</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>198</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>136</td>\n",
       "      <td>140</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008306</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>198</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>135</td>\n",
       "      <td>138</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>170</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>137</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132</td>\n",
       "      <td>0.006515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>136</td>\n",
       "      <td>138</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LB        AC   FM        UC        DL   DS   DP  ASTV  MSTV  ALTV ...   \\\n",
       "0  120  0.000000  0.0  0.000000  0.000000  0.0  0.0    73   0.5    43 ...    \n",
       "1  132  0.006380  0.0  0.006380  0.003190  0.0  0.0    17   2.1     0 ...    \n",
       "2  133  0.003322  0.0  0.008306  0.003322  0.0  0.0    16   2.1     0 ...    \n",
       "3  134  0.002561  0.0  0.007682  0.002561  0.0  0.0    16   2.4     0 ...    \n",
       "4  132  0.006515  0.0  0.008143  0.000000  0.0  0.0    16   2.4     0 ...    \n",
       "\n",
       "   Min  Max  Nmax  Nzeros  Mode  Mean  Median  Variance  Tendency  NSP  \n",
       "0   62  126     2       0   120   137     121        73         1    2  \n",
       "1   68  198     6       1   141   136     140        12         0    1  \n",
       "2   68  198     5       1   141   135     138        13         0    1  \n",
       "3   53  170    11       0   137   134     137        13         1    1  \n",
       "4   53  170     9       0   137   136     138        11         1    1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the head of the data\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train and test data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x= data.drop('NSP', axis= 1)\n",
    "y= data['NSP']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train= y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.67549718e-01,  2.39568986e-05,  2.99461265e-06, ...,\n",
       "         3.88703659e-01,  1.87741223e-01,  2.64424258e-03],\n",
       "       [ 3.75556761e-01,  4.74907485e-06,  3.79925894e-05, ...,\n",
       "         3.91204959e-01,  1.56481984e-02, -3.12963967e-03],\n",
       "       [ 4.03276404e-01,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         4.06477010e-01,  6.40121276e-03,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 2.91289229e-01,  6.96253549e-06,  0.00000000e+00, ...,\n",
       "         2.83344977e-01,  9.26829364e-02, -2.64808390e-03],\n",
       "       [ 3.54945168e-01,  2.49835720e-05,  1.10641531e-04, ...,\n",
       "         3.98630727e-01,  4.64159066e-02,  2.73034745e-03],\n",
       "       [ 3.76397501e-01,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         4.09459444e-01,  0.00000000e+00,  2.54322636e-03]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing of the data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x_train= preprocessing.normalize(x_train)\n",
    "x_test= preprocessing.normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the keras sequential model\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the structure of the model\n",
    "# we have 3 hidden layer with input and output layer\n",
    "\n",
    "model.add(Dense(50, activation= 'relu', input_shape= (21,)))\n",
    "model.add(Dense(42, activation= 'relu'))\n",
    "model.add(Dense(35, activation= 'relu'))\n",
    "model.add(Dense(25, activation= 'relu'))\n",
    "model.add(Dense(3, activation= 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Train on 1275 samples, validate on 319 samples\n",
      "Epoch 1/250\n",
      "1275/1275 [==============================] - 1s 1ms/step - loss: 1.0066 - acc: 0.7192 - val_loss: 0.9230 - val_acc: 0.7743\n",
      "Epoch 2/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.8597 - acc: 0.7749 - val_loss: 0.7979 - val_acc: 0.7743\n",
      "Epoch 3/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.7687 - acc: 0.7749 - val_loss: 0.7351 - val_acc: 0.7743\n",
      "Epoch 4/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.7238 - acc: 0.7749 - val_loss: 0.7053 - val_acc: 0.7743\n",
      "Epoch 5/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.7033 - acc: 0.7749 - val_loss: 0.6934 - val_acc: 0.7743\n",
      "Epoch 6/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.6937 - acc: 0.7749 - val_loss: 0.6877 - val_acc: 0.7743\n",
      "Epoch 7/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.6884 - acc: 0.7749 - val_loss: 0.6848 - val_acc: 0.7743\n",
      "Epoch 8/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.6850 - acc: 0.7749 - val_loss: 0.6831 - val_acc: 0.7743\n",
      "Epoch 9/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.6823 - acc: 0.7749 - val_loss: 0.6819 - val_acc: 0.7743\n",
      "Epoch 10/250\n",
      "1275/1275 [==============================] - 0s 98us/step - loss: 0.6799 - acc: 0.7749 - val_loss: 0.6808 - val_acc: 0.7743\n",
      "Epoch 11/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.6780 - acc: 0.7749 - val_loss: 0.6800 - val_acc: 0.7743\n",
      "Epoch 12/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.6763 - acc: 0.7749 - val_loss: 0.6792 - val_acc: 0.7743\n",
      "Epoch 13/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.6750 - acc: 0.7749 - val_loss: 0.6784 - val_acc: 0.7743\n",
      "Epoch 14/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.6737 - acc: 0.7749 - val_loss: 0.6779 - val_acc: 0.7743\n",
      "Epoch 15/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.6726 - acc: 0.7749 - val_loss: 0.6776 - val_acc: 0.7743\n",
      "Epoch 16/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.6717 - acc: 0.7749 - val_loss: 0.6770 - val_acc: 0.7743\n",
      "Epoch 17/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.6706 - acc: 0.7749 - val_loss: 0.6765 - val_acc: 0.7743\n",
      "Epoch 18/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.6696 - acc: 0.7749 - val_loss: 0.6760 - val_acc: 0.7743\n",
      "Epoch 19/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.6690 - acc: 0.7749 - val_loss: 0.6756 - val_acc: 0.7743\n",
      "Epoch 20/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.6680 - acc: 0.7749 - val_loss: 0.6752 - val_acc: 0.7743\n",
      "Epoch 21/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.6671 - acc: 0.7749 - val_loss: 0.6745 - val_acc: 0.7743\n",
      "Epoch 22/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.6662 - acc: 0.7749 - val_loss: 0.6739 - val_acc: 0.7743\n",
      "Epoch 23/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.6653 - acc: 0.7749 - val_loss: 0.6732 - val_acc: 0.7743\n",
      "Epoch 24/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.6645 - acc: 0.7749 - val_loss: 0.6726 - val_acc: 0.7743\n",
      "Epoch 25/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.6634 - acc: 0.7749 - val_loss: 0.6717 - val_acc: 0.7743\n",
      "Epoch 26/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.6627 - acc: 0.7749 - val_loss: 0.6710 - val_acc: 0.7743\n",
      "Epoch 27/250\n",
      "1275/1275 [==============================] - 0s 159us/step - loss: 0.6617 - acc: 0.7749 - val_loss: 0.6704 - val_acc: 0.7743\n",
      "Epoch 28/250\n",
      "1275/1275 [==============================] - 0s 112us/step - loss: 0.6607 - acc: 0.7749 - val_loss: 0.6694 - val_acc: 0.7743\n",
      "Epoch 29/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.6598 - acc: 0.7749 - val_loss: 0.6686 - val_acc: 0.7743\n",
      "Epoch 30/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.6587 - acc: 0.7749 - val_loss: 0.6676 - val_acc: 0.7743\n",
      "Epoch 31/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.6577 - acc: 0.7749 - val_loss: 0.6665 - val_acc: 0.7743\n",
      "Epoch 32/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.6568 - acc: 0.7749 - val_loss: 0.6655 - val_acc: 0.7743\n",
      "Epoch 33/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.6554 - acc: 0.7749 - val_loss: 0.6645 - val_acc: 0.7743\n",
      "Epoch 34/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.6543 - acc: 0.7749 - val_loss: 0.6632 - val_acc: 0.7743\n",
      "Epoch 35/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.6530 - acc: 0.7749 - val_loss: 0.6618 - val_acc: 0.7743\n",
      "Epoch 36/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.6518 - acc: 0.7749 - val_loss: 0.6602 - val_acc: 0.7743\n",
      "Epoch 37/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.6498 - acc: 0.7749 - val_loss: 0.6584 - val_acc: 0.7743\n",
      "Epoch 38/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.6480 - acc: 0.7749 - val_loss: 0.6569 - val_acc: 0.7743\n",
      "Epoch 39/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.6467 - acc: 0.7749 - val_loss: 0.6552 - val_acc: 0.7743\n",
      "Epoch 40/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.6450 - acc: 0.7749 - val_loss: 0.6536 - val_acc: 0.7743\n",
      "Epoch 41/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.6430 - acc: 0.7749 - val_loss: 0.6521 - val_acc: 0.7743\n",
      "Epoch 42/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.6415 - acc: 0.7749 - val_loss: 0.6499 - val_acc: 0.7743\n",
      "Epoch 43/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.6396 - acc: 0.7749 - val_loss: 0.6483 - val_acc: 0.7743\n",
      "Epoch 44/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.6376 - acc: 0.7749 - val_loss: 0.6458 - val_acc: 0.7743\n",
      "Epoch 45/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.6356 - acc: 0.7749 - val_loss: 0.6438 - val_acc: 0.7743\n",
      "Epoch 46/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.6339 - acc: 0.7749 - val_loss: 0.6415 - val_acc: 0.7743\n",
      "Epoch 47/250\n",
      "1275/1275 [==============================] - 0s 104us/step - loss: 0.6312 - acc: 0.7749 - val_loss: 0.6391 - val_acc: 0.7743\n",
      "Epoch 48/250\n",
      "1275/1275 [==============================] - 0s 145us/step - loss: 0.6285 - acc: 0.7749 - val_loss: 0.6366 - val_acc: 0.7743\n",
      "Epoch 49/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.6262 - acc: 0.7749 - val_loss: 0.6332 - val_acc: 0.7743\n",
      "Epoch 50/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.6228 - acc: 0.7749 - val_loss: 0.6303 - val_acc: 0.7743\n",
      "Epoch 51/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.6197 - acc: 0.7749 - val_loss: 0.6267 - val_acc: 0.7743\n",
      "Epoch 52/250\n",
      "1275/1275 [==============================] - 0s 108us/step - loss: 0.6169 - acc: 0.7749 - val_loss: 0.6232 - val_acc: 0.7743\n",
      "Epoch 53/250\n",
      "1275/1275 [==============================] - 0s 104us/step - loss: 0.6130 - acc: 0.7749 - val_loss: 0.6197 - val_acc: 0.7743\n",
      "Epoch 54/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.6093 - acc: 0.7749 - val_loss: 0.6166 - val_acc: 0.7743\n",
      "Epoch 55/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.6068 - acc: 0.7749 - val_loss: 0.6121 - val_acc: 0.7743\n",
      "Epoch 56/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.6031 - acc: 0.7749 - val_loss: 0.6082 - val_acc: 0.7743\n",
      "Epoch 57/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.5987 - acc: 0.7749 - val_loss: 0.6052 - val_acc: 0.7743\n",
      "Epoch 58/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.5934 - acc: 0.7749 - val_loss: 0.5999 - val_acc: 0.7743\n",
      "Epoch 59/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.5883 - acc: 0.7749 - val_loss: 0.5947 - val_acc: 0.7743\n",
      "Epoch 60/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.5851 - acc: 0.7749 - val_loss: 0.5885 - val_acc: 0.7743\n",
      "Epoch 61/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.5795 - acc: 0.7749 - val_loss: 0.5840 - val_acc: 0.7743\n",
      "Epoch 62/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.5744 - acc: 0.7749 - val_loss: 0.5769 - val_acc: 0.7743\n",
      "Epoch 63/250\n",
      "1275/1275 [==============================] - 0s 64us/step - loss: 0.5684 - acc: 0.7749 - val_loss: 0.5701 - val_acc: 0.7743\n",
      "Epoch 64/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.5628 - acc: 0.7749 - val_loss: 0.5686 - val_acc: 0.7743\n",
      "Epoch 65/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.5582 - acc: 0.7749 - val_loss: 0.5583 - val_acc: 0.7743\n",
      "Epoch 66/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.5507 - acc: 0.7741 - val_loss: 0.5500 - val_acc: 0.7743\n",
      "Epoch 67/250\n",
      "1275/1275 [==============================] - 0s 66us/step - loss: 0.5446 - acc: 0.7733 - val_loss: 0.5433 - val_acc: 0.7743\n",
      "Epoch 68/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.5395 - acc: 0.7725 - val_loss: 0.5418 - val_acc: 0.7743\n",
      "Epoch 69/250\n",
      "1275/1275 [==============================] - 0s 65us/step - loss: 0.5336 - acc: 0.7788 - val_loss: 0.5305 - val_acc: 0.7712\n",
      "Epoch 70/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.5264 - acc: 0.7788 - val_loss: 0.5231 - val_acc: 0.7649\n",
      "Epoch 71/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.5200 - acc: 0.7859 - val_loss: 0.5190 - val_acc: 0.7618\n",
      "Epoch 72/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.5150 - acc: 0.7851 - val_loss: 0.5134 - val_acc: 0.7618\n",
      "Epoch 73/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.5089 - acc: 0.7882 - val_loss: 0.5039 - val_acc: 0.7680\n",
      "Epoch 74/250\n",
      "1275/1275 [==============================] - 0s 66us/step - loss: 0.5063 - acc: 0.7922 - val_loss: 0.4977 - val_acc: 0.7712\n",
      "Epoch 75/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.4982 - acc: 0.7937 - val_loss: 0.4911 - val_acc: 0.7712\n",
      "Epoch 76/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.4940 - acc: 0.7898 - val_loss: 0.4890 - val_acc: 0.7743\n",
      "Epoch 77/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.4961 - acc: 0.7929 - val_loss: 0.4789 - val_acc: 0.7743\n",
      "Epoch 78/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.4826 - acc: 0.7961 - val_loss: 0.4726 - val_acc: 0.7743\n",
      "Epoch 79/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.4826 - acc: 0.8031 - val_loss: 0.4756 - val_acc: 0.7712\n",
      "Epoch 80/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.4776 - acc: 0.8039 - val_loss: 0.4620 - val_acc: 0.7806\n",
      "Epoch 81/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.4711 - acc: 0.8055 - val_loss: 0.4623 - val_acc: 0.7962\n",
      "Epoch 82/250\n",
      "1275/1275 [==============================] - 0s 95us/step - loss: 0.4676 - acc: 0.8102 - val_loss: 0.4733 - val_acc: 0.7774\n",
      "Epoch 83/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.4622 - acc: 0.8165 - val_loss: 0.4479 - val_acc: 0.7900\n",
      "Epoch 84/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.4594 - acc: 0.8149 - val_loss: 0.4446 - val_acc: 0.8056\n",
      "Epoch 85/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.4561 - acc: 0.8188 - val_loss: 0.4389 - val_acc: 0.7962\n",
      "Epoch 86/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.4557 - acc: 0.8212 - val_loss: 0.4448 - val_acc: 0.7868\n",
      "Epoch 87/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.4564 - acc: 0.8220 - val_loss: 0.4315 - val_acc: 0.8245\n",
      "Epoch 88/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.4488 - acc: 0.8267 - val_loss: 0.4293 - val_acc: 0.7994\n",
      "Epoch 89/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.4444 - acc: 0.8282 - val_loss: 0.4335 - val_acc: 0.8401\n",
      "Epoch 90/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.4433 - acc: 0.8290 - val_loss: 0.4219 - val_acc: 0.8213\n",
      "Epoch 91/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.4421 - acc: 0.8259 - val_loss: 0.4355 - val_acc: 0.8495\n",
      "Epoch 92/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.4377 - acc: 0.8306 - val_loss: 0.4348 - val_acc: 0.7931\n",
      "Epoch 93/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.4351 - acc: 0.8361 - val_loss: 0.4157 - val_acc: 0.8150\n",
      "Epoch 94/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.4316 - acc: 0.8306 - val_loss: 0.4151 - val_acc: 0.8401\n",
      "Epoch 95/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.4286 - acc: 0.8353 - val_loss: 0.4065 - val_acc: 0.8276\n",
      "Epoch 96/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.4271 - acc: 0.8376 - val_loss: 0.4042 - val_acc: 0.8276\n",
      "Epoch 97/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.4257 - acc: 0.8361 - val_loss: 0.4122 - val_acc: 0.8119\n",
      "Epoch 98/250\n",
      "1275/1275 [==============================] - 0s 131us/step - loss: 0.4277 - acc: 0.8322 - val_loss: 0.4004 - val_acc: 0.8307\n",
      "Epoch 99/250\n",
      "1275/1275 [==============================] - 0s 127us/step - loss: 0.4236 - acc: 0.8369 - val_loss: 0.4185 - val_acc: 0.8056\n",
      "Epoch 100/250\n",
      "1275/1275 [==============================] - 0s 163us/step - loss: 0.4258 - acc: 0.8431 - val_loss: 0.4131 - val_acc: 0.8088\n",
      "Epoch 101/250\n",
      "1275/1275 [==============================] - 0s 108us/step - loss: 0.4178 - acc: 0.8376 - val_loss: 0.3950 - val_acc: 0.8276\n",
      "Epoch 102/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.4215 - acc: 0.8337 - val_loss: 0.3948 - val_acc: 0.8245\n",
      "Epoch 103/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.4127 - acc: 0.8369 - val_loss: 0.3972 - val_acc: 0.8182\n",
      "Epoch 104/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.4147 - acc: 0.8337 - val_loss: 0.3884 - val_acc: 0.8339\n",
      "Epoch 105/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.4069 - acc: 0.8400 - val_loss: 0.3873 - val_acc: 0.8339\n",
      "Epoch 106/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.4076 - acc: 0.8392 - val_loss: 0.3917 - val_acc: 0.8495\n",
      "Epoch 107/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.4052 - acc: 0.8384 - val_loss: 0.3840 - val_acc: 0.8307\n",
      "Epoch 108/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.4119 - acc: 0.8392 - val_loss: 0.3813 - val_acc: 0.8370\n",
      "Epoch 109/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.4052 - acc: 0.8416 - val_loss: 0.4087 - val_acc: 0.8088\n",
      "Epoch 110/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3993 - acc: 0.8463 - val_loss: 0.3791 - val_acc: 0.8558\n",
      "Epoch 111/250\n",
      "1275/1275 [==============================] - 0s 116us/step - loss: 0.3982 - acc: 0.8439 - val_loss: 0.3821 - val_acc: 0.8589\n",
      "Epoch 112/250\n",
      "1275/1275 [==============================] - 0s 155us/step - loss: 0.3986 - acc: 0.8463 - val_loss: 0.3772 - val_acc: 0.8495\n",
      "Epoch 113/250\n",
      "1275/1275 [==============================] - 0s 97us/step - loss: 0.4028 - acc: 0.8455 - val_loss: 0.3735 - val_acc: 0.8339\n",
      "Epoch 114/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.4005 - acc: 0.8455 - val_loss: 0.3882 - val_acc: 0.8652\n",
      "Epoch 115/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3988 - acc: 0.8494 - val_loss: 0.3723 - val_acc: 0.8433\n",
      "Epoch 116/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3938 - acc: 0.8502 - val_loss: 0.3859 - val_acc: 0.8276\n",
      "Epoch 117/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3881 - acc: 0.8463 - val_loss: 0.3716 - val_acc: 0.8558\n",
      "Epoch 118/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3924 - acc: 0.8494 - val_loss: 0.3794 - val_acc: 0.8589\n",
      "Epoch 119/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3895 - acc: 0.8494 - val_loss: 0.3744 - val_acc: 0.8621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3890 - acc: 0.8533 - val_loss: 0.4672 - val_acc: 0.8276\n",
      "Epoch 121/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3943 - acc: 0.8463 - val_loss: 0.3757 - val_acc: 0.8339\n",
      "Epoch 122/250\n",
      "1275/1275 [==============================] - 0s 70us/step - loss: 0.3960 - acc: 0.8408 - val_loss: 0.3650 - val_acc: 0.8589\n",
      "Epoch 123/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3909 - acc: 0.8486 - val_loss: 0.3679 - val_acc: 0.8464\n",
      "Epoch 124/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3899 - acc: 0.8447 - val_loss: 0.3616 - val_acc: 0.8527\n",
      "Epoch 125/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.3885 - acc: 0.8557 - val_loss: 0.3756 - val_acc: 0.8339\n",
      "Epoch 126/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3921 - acc: 0.8463 - val_loss: 0.3638 - val_acc: 0.8464\n",
      "Epoch 127/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3884 - acc: 0.8510 - val_loss: 0.3626 - val_acc: 0.8464\n",
      "Epoch 128/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3827 - acc: 0.8486 - val_loss: 0.3600 - val_acc: 0.8589\n",
      "Epoch 129/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3848 - acc: 0.8471 - val_loss: 0.3677 - val_acc: 0.8401\n",
      "Epoch 130/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3810 - acc: 0.8502 - val_loss: 0.3574 - val_acc: 0.8495\n",
      "Epoch 131/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3842 - acc: 0.8494 - val_loss: 0.3630 - val_acc: 0.8621\n",
      "Epoch 132/250\n",
      "1275/1275 [==============================] - 0s 70us/step - loss: 0.3799 - acc: 0.8518 - val_loss: 0.3818 - val_acc: 0.8746\n",
      "Epoch 133/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3830 - acc: 0.8533 - val_loss: 0.3594 - val_acc: 0.8464\n",
      "Epoch 134/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3814 - acc: 0.8486 - val_loss: 0.3639 - val_acc: 0.8495\n",
      "Epoch 135/250\n",
      "1275/1275 [==============================] - 0s 70us/step - loss: 0.3863 - acc: 0.8471 - val_loss: 0.3671 - val_acc: 0.8746\n",
      "Epoch 136/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3800 - acc: 0.8565 - val_loss: 0.3560 - val_acc: 0.8464\n",
      "Epoch 137/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3803 - acc: 0.8525 - val_loss: 0.3540 - val_acc: 0.8589\n",
      "Epoch 138/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.3788 - acc: 0.8549 - val_loss: 0.3581 - val_acc: 0.8715\n",
      "Epoch 139/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3754 - acc: 0.8565 - val_loss: 0.3540 - val_acc: 0.8589\n",
      "Epoch 140/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3743 - acc: 0.8502 - val_loss: 0.3547 - val_acc: 0.8652\n",
      "Epoch 141/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3732 - acc: 0.8627 - val_loss: 0.3595 - val_acc: 0.8527\n",
      "Epoch 142/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3781 - acc: 0.8502 - val_loss: 0.3505 - val_acc: 0.8621\n",
      "Epoch 143/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3747 - acc: 0.8573 - val_loss: 0.3720 - val_acc: 0.8339\n",
      "Epoch 144/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3785 - acc: 0.8455 - val_loss: 0.4035 - val_acc: 0.8150\n",
      "Epoch 145/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3723 - acc: 0.8573 - val_loss: 0.3566 - val_acc: 0.8495\n",
      "Epoch 146/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3770 - acc: 0.8557 - val_loss: 0.3904 - val_acc: 0.8213\n",
      "Epoch 147/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3753 - acc: 0.8573 - val_loss: 0.3493 - val_acc: 0.8558\n",
      "Epoch 148/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3761 - acc: 0.8533 - val_loss: 0.3492 - val_acc: 0.8683\n",
      "Epoch 149/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3687 - acc: 0.8557 - val_loss: 0.3534 - val_acc: 0.8495\n",
      "Epoch 150/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3714 - acc: 0.8502 - val_loss: 0.3571 - val_acc: 0.8809\n",
      "Epoch 151/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3733 - acc: 0.8502 - val_loss: 0.3720 - val_acc: 0.8401\n",
      "Epoch 152/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3750 - acc: 0.8502 - val_loss: 0.3606 - val_acc: 0.8464\n",
      "Epoch 153/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3724 - acc: 0.8612 - val_loss: 0.3567 - val_acc: 0.8495\n",
      "Epoch 154/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3681 - acc: 0.8573 - val_loss: 0.3808 - val_acc: 0.8401\n",
      "Epoch 155/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3657 - acc: 0.8518 - val_loss: 0.3686 - val_acc: 0.8746\n",
      "Epoch 156/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3689 - acc: 0.8549 - val_loss: 0.3921 - val_acc: 0.8245\n",
      "Epoch 157/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3718 - acc: 0.8525 - val_loss: 0.3482 - val_acc: 0.8715\n",
      "Epoch 158/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3707 - acc: 0.8525 - val_loss: 0.3488 - val_acc: 0.8621\n",
      "Epoch 159/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3725 - acc: 0.8518 - val_loss: 0.3571 - val_acc: 0.8589\n",
      "Epoch 160/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.3668 - acc: 0.8580 - val_loss: 0.3475 - val_acc: 0.8589\n",
      "Epoch 161/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3676 - acc: 0.8596 - val_loss: 0.3519 - val_acc: 0.8527\n",
      "Epoch 162/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3667 - acc: 0.8549 - val_loss: 0.3555 - val_acc: 0.8495\n",
      "Epoch 163/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3686 - acc: 0.8510 - val_loss: 0.3445 - val_acc: 0.8746\n",
      "Epoch 164/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3742 - acc: 0.8525 - val_loss: 0.3825 - val_acc: 0.8339\n",
      "Epoch 165/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3672 - acc: 0.8604 - val_loss: 0.3465 - val_acc: 0.8777\n",
      "Epoch 166/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3725 - acc: 0.8549 - val_loss: 0.3706 - val_acc: 0.8683\n",
      "Epoch 167/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3755 - acc: 0.8525 - val_loss: 0.3478 - val_acc: 0.8840\n",
      "Epoch 168/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3634 - acc: 0.8424 - val_loss: 0.3454 - val_acc: 0.8621\n",
      "Epoch 169/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3689 - acc: 0.8533 - val_loss: 0.3494 - val_acc: 0.8558\n",
      "Epoch 170/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3687 - acc: 0.8510 - val_loss: 0.3469 - val_acc: 0.8652\n",
      "Epoch 171/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3664 - acc: 0.8549 - val_loss: 0.3433 - val_acc: 0.8621\n",
      "Epoch 172/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3639 - acc: 0.8549 - val_loss: 0.3749 - val_acc: 0.8464\n",
      "Epoch 173/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3705 - acc: 0.8518 - val_loss: 0.3576 - val_acc: 0.8715\n",
      "Epoch 174/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3592 - acc: 0.8573 - val_loss: 0.3450 - val_acc: 0.8589\n",
      "Epoch 175/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3623 - acc: 0.8612 - val_loss: 0.3442 - val_acc: 0.8683\n",
      "Epoch 176/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3616 - acc: 0.8557 - val_loss: 0.3619 - val_acc: 0.8495\n",
      "Epoch 177/250\n",
      "1275/1275 [==============================] - 0s 108us/step - loss: 0.3744 - acc: 0.8502 - val_loss: 0.3494 - val_acc: 0.8683\n",
      "Epoch 178/250\n",
      "1275/1275 [==============================] - 0s 95us/step - loss: 0.3668 - acc: 0.8494 - val_loss: 0.3631 - val_acc: 0.8495\n",
      "Epoch 179/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3653 - acc: 0.8533 - val_loss: 0.3419 - val_acc: 0.8715\n",
      "Epoch 180/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3675 - acc: 0.8525 - val_loss: 0.3420 - val_acc: 0.8715\n",
      "Epoch 181/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3653 - acc: 0.8502 - val_loss: 0.3420 - val_acc: 0.8683\n",
      "Epoch 182/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3639 - acc: 0.8541 - val_loss: 0.3410 - val_acc: 0.8840\n",
      "Epoch 183/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3639 - acc: 0.8588 - val_loss: 0.3514 - val_acc: 0.8715\n",
      "Epoch 184/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3663 - acc: 0.8502 - val_loss: 0.3485 - val_acc: 0.8652\n",
      "Epoch 185/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3611 - acc: 0.8533 - val_loss: 0.4060 - val_acc: 0.8213\n",
      "Epoch 186/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3624 - acc: 0.8549 - val_loss: 0.3402 - val_acc: 0.8683\n",
      "Epoch 187/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.3618 - acc: 0.8596 - val_loss: 0.3699 - val_acc: 0.8558\n",
      "Epoch 188/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3637 - acc: 0.8518 - val_loss: 0.3356 - val_acc: 0.8809\n",
      "Epoch 189/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3607 - acc: 0.8620 - val_loss: 0.3416 - val_acc: 0.8715\n",
      "Epoch 190/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3584 - acc: 0.8525 - val_loss: 0.3583 - val_acc: 0.8527\n",
      "Epoch 191/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3665 - acc: 0.8588 - val_loss: 0.3419 - val_acc: 0.8746\n",
      "Epoch 192/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3600 - acc: 0.8565 - val_loss: 0.3400 - val_acc: 0.8777\n",
      "Epoch 193/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3545 - acc: 0.8588 - val_loss: 0.3399 - val_acc: 0.8840\n",
      "Epoch 194/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3608 - acc: 0.8533 - val_loss: 0.3403 - val_acc: 0.8652\n",
      "Epoch 195/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3603 - acc: 0.8612 - val_loss: 0.3385 - val_acc: 0.8871\n",
      "Epoch 196/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3719 - acc: 0.8604 - val_loss: 0.3423 - val_acc: 0.8683\n",
      "Epoch 197/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3593 - acc: 0.8533 - val_loss: 0.3664 - val_acc: 0.8464\n",
      "Epoch 198/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3673 - acc: 0.8525 - val_loss: 0.3560 - val_acc: 0.8621\n",
      "Epoch 199/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3637 - acc: 0.8573 - val_loss: 0.3396 - val_acc: 0.8746\n",
      "Epoch 200/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3637 - acc: 0.8557 - val_loss: 0.3451 - val_acc: 0.8589\n",
      "Epoch 201/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3663 - acc: 0.8494 - val_loss: 0.3416 - val_acc: 0.8621\n",
      "Epoch 202/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3616 - acc: 0.8525 - val_loss: 0.3408 - val_acc: 0.8777\n",
      "Epoch 203/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3614 - acc: 0.8596 - val_loss: 0.3379 - val_acc: 0.8715\n",
      "Epoch 204/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3641 - acc: 0.8541 - val_loss: 0.3608 - val_acc: 0.8621\n",
      "Epoch 205/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3583 - acc: 0.8502 - val_loss: 0.3333 - val_acc: 0.8777\n",
      "Epoch 206/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3591 - acc: 0.8596 - val_loss: 0.3547 - val_acc: 0.8558\n",
      "Epoch 207/250\n",
      "1275/1275 [==============================] - 0s 106us/step - loss: 0.3620 - acc: 0.8620 - val_loss: 0.3366 - val_acc: 0.8746\n",
      "Epoch 208/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3549 - acc: 0.8557 - val_loss: 0.3722 - val_acc: 0.8370\n",
      "Epoch 209/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3652 - acc: 0.8471 - val_loss: 0.3491 - val_acc: 0.8746\n",
      "Epoch 210/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3607 - acc: 0.8549 - val_loss: 0.3443 - val_acc: 0.8746\n",
      "Epoch 211/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3565 - acc: 0.8557 - val_loss: 0.3603 - val_acc: 0.8495\n",
      "Epoch 212/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3605 - acc: 0.8573 - val_loss: 0.3594 - val_acc: 0.8558\n",
      "Epoch 213/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3581 - acc: 0.8635 - val_loss: 0.3480 - val_acc: 0.8527\n",
      "Epoch 214/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3609 - acc: 0.8588 - val_loss: 0.3542 - val_acc: 0.8527\n",
      "Epoch 215/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3589 - acc: 0.8557 - val_loss: 0.3397 - val_acc: 0.8871\n",
      "Epoch 216/250\n",
      "1275/1275 [==============================] - 0s 181us/step - loss: 0.3576 - acc: 0.8596 - val_loss: 0.3536 - val_acc: 0.8527\n",
      "Epoch 217/250\n",
      "1275/1275 [==============================] - 0s 127us/step - loss: 0.3590 - acc: 0.8573 - val_loss: 0.3436 - val_acc: 0.8777\n",
      "Epoch 218/250\n",
      "1275/1275 [==============================] - 0s 146us/step - loss: 0.3612 - acc: 0.8565 - val_loss: 0.3359 - val_acc: 0.8809\n",
      "Epoch 219/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3580 - acc: 0.8588 - val_loss: 0.3364 - val_acc: 0.8840\n",
      "Epoch 220/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3593 - acc: 0.8541 - val_loss: 0.3323 - val_acc: 0.8809\n",
      "Epoch 221/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3569 - acc: 0.8643 - val_loss: 0.3429 - val_acc: 0.8777\n",
      "Epoch 222/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3603 - acc: 0.8565 - val_loss: 0.3450 - val_acc: 0.8777\n",
      "Epoch 223/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3569 - acc: 0.8549 - val_loss: 0.3335 - val_acc: 0.8683\n",
      "Epoch 224/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3569 - acc: 0.8573 - val_loss: 0.3449 - val_acc: 0.8715\n",
      "Epoch 225/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3592 - acc: 0.8549 - val_loss: 0.3324 - val_acc: 0.8840\n",
      "Epoch 226/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3597 - acc: 0.8549 - val_loss: 0.3481 - val_acc: 0.8652\n",
      "Epoch 227/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3635 - acc: 0.8588 - val_loss: 0.3372 - val_acc: 0.8809\n",
      "Epoch 228/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3557 - acc: 0.8604 - val_loss: 0.3313 - val_acc: 0.8840\n",
      "Epoch 229/250\n",
      "1275/1275 [==============================] - 0s 108us/step - loss: 0.3557 - acc: 0.8580 - val_loss: 0.3420 - val_acc: 0.8715\n",
      "Epoch 230/250\n",
      "1275/1275 [==============================] - 0s 110us/step - loss: 0.3600 - acc: 0.8533 - val_loss: 0.3295 - val_acc: 0.8777\n",
      "Epoch 231/250\n",
      "1275/1275 [==============================] - 0s 102us/step - loss: 0.3652 - acc: 0.8525 - val_loss: 0.3378 - val_acc: 0.8871\n",
      "Epoch 232/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3559 - acc: 0.8580 - val_loss: 0.3414 - val_acc: 0.8840\n",
      "Epoch 233/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3542 - acc: 0.8557 - val_loss: 0.3360 - val_acc: 0.8652\n",
      "Epoch 234/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3549 - acc: 0.8588 - val_loss: 0.3361 - val_acc: 0.8809\n",
      "Epoch 235/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3604 - acc: 0.8580 - val_loss: 0.3525 - val_acc: 0.8621\n",
      "Epoch 236/250\n",
      "1275/1275 [==============================] - 0s 130us/step - loss: 0.3524 - acc: 0.8573 - val_loss: 0.3454 - val_acc: 0.8589\n",
      "Epoch 237/250\n",
      "1275/1275 [==============================] - 0s 109us/step - loss: 0.3557 - acc: 0.8588 - val_loss: 0.3331 - val_acc: 0.8840\n",
      "Epoch 238/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3603 - acc: 0.8533 - val_loss: 0.3462 - val_acc: 0.8652\n",
      "Epoch 239/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3542 - acc: 0.8612 - val_loss: 0.3803 - val_acc: 0.8401\n",
      "Epoch 240/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3609 - acc: 0.8659 - val_loss: 0.3261 - val_acc: 0.8809\n",
      "Epoch 241/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3497 - acc: 0.8557 - val_loss: 0.3418 - val_acc: 0.8715\n",
      "Epoch 242/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.3525 - acc: 0.8604 - val_loss: 0.3318 - val_acc: 0.8809\n",
      "Epoch 243/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.3571 - acc: 0.8565 - val_loss: 0.3406 - val_acc: 0.8871\n",
      "Epoch 244/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.3522 - acc: 0.8573 - val_loss: 0.3328 - val_acc: 0.8871\n",
      "Epoch 245/250\n",
      "1275/1275 [==============================] - 0s 129us/step - loss: 0.3518 - acc: 0.8565 - val_loss: 0.3496 - val_acc: 0.8652\n",
      "Epoch 246/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3517 - acc: 0.8596 - val_loss: 0.3377 - val_acc: 0.8715\n",
      "Epoch 247/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3548 - acc: 0.8549 - val_loss: 0.3647 - val_acc: 0.8589\n",
      "Epoch 248/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3573 - acc: 0.8573 - val_loss: 0.3341 - val_acc: 0.8840\n",
      "Epoch 249/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3532 - acc: 0.8580 - val_loss: 0.3948 - val_acc: 0.8339\n",
      "Epoch 250/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3573 - acc: 0.8525 - val_loss: 0.3277 - val_acc: 0.8840\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.030000\n",
      "\n",
      "Train on 1275 samples, validate on 319 samples\n",
      "Epoch 1/250\n",
      "1275/1275 [==============================] - 1s 1ms/step - loss: 0.4237 - acc: 0.8565 - val_loss: 0.4797 - val_acc: 0.8025\n",
      "Epoch 2/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.4322 - acc: 0.8322 - val_loss: 0.6632 - val_acc: 0.7743\n",
      "Epoch 3/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.4133 - acc: 0.8384 - val_loss: 0.4531 - val_acc: 0.7994\n",
      "Epoch 4/250\n",
      "1275/1275 [==============================] - 0s 70us/step - loss: 0.4101 - acc: 0.8408 - val_loss: 0.5037 - val_acc: 0.7931\n",
      "Epoch 5/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3876 - acc: 0.8471 - val_loss: 0.3575 - val_acc: 0.8433\n",
      "Epoch 6/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.4287 - acc: 0.8369 - val_loss: 0.3397 - val_acc: 0.8401\n",
      "Epoch 7/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3891 - acc: 0.8518 - val_loss: 0.3332 - val_acc: 0.8871\n",
      "Epoch 8/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3909 - acc: 0.8392 - val_loss: 0.3704 - val_acc: 0.8370\n",
      "Epoch 9/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.3762 - acc: 0.8518 - val_loss: 0.3272 - val_acc: 0.8871\n",
      "Epoch 10/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3772 - acc: 0.8494 - val_loss: 0.3304 - val_acc: 0.8840\n",
      "Epoch 11/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.4083 - acc: 0.8447 - val_loss: 0.3559 - val_acc: 0.8401\n",
      "Epoch 12/250\n",
      "1275/1275 [==============================] - 0s 162us/step - loss: 0.3641 - acc: 0.8557 - val_loss: 0.3486 - val_acc: 0.8809\n",
      "Epoch 13/250\n",
      "1275/1275 [==============================] - 0s 113us/step - loss: 0.3901 - acc: 0.8486 - val_loss: 0.5662 - val_acc: 0.7868\n",
      "Epoch 14/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3732 - acc: 0.8612 - val_loss: 0.3169 - val_acc: 0.8809\n",
      "Epoch 15/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3853 - acc: 0.8408 - val_loss: 0.3542 - val_acc: 0.8464\n",
      "Epoch 16/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3852 - acc: 0.8494 - val_loss: 0.3410 - val_acc: 0.8966\n",
      "Epoch 17/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3805 - acc: 0.8455 - val_loss: 0.3397 - val_acc: 0.8558\n",
      "Epoch 18/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3734 - acc: 0.8541 - val_loss: 0.3310 - val_acc: 0.8683\n",
      "Epoch 19/250\n",
      "1275/1275 [==============================] - 0s 134us/step - loss: 0.3752 - acc: 0.8518 - val_loss: 0.3476 - val_acc: 0.8715\n",
      "Epoch 20/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3767 - acc: 0.8549 - val_loss: 0.3324 - val_acc: 0.8777\n",
      "Epoch 21/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3654 - acc: 0.8667 - val_loss: 0.3552 - val_acc: 0.8495\n",
      "Epoch 22/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3932 - acc: 0.8471 - val_loss: 0.3822 - val_acc: 0.8276\n",
      "Epoch 23/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3731 - acc: 0.8408 - val_loss: 0.3166 - val_acc: 0.8840\n",
      "Epoch 24/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3806 - acc: 0.8463 - val_loss: 0.3237 - val_acc: 0.8809\n",
      "Epoch 25/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3779 - acc: 0.8455 - val_loss: 0.3470 - val_acc: 0.8433\n",
      "Epoch 26/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3596 - acc: 0.8549 - val_loss: 0.3324 - val_acc: 0.8934\n",
      "Epoch 27/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3640 - acc: 0.8596 - val_loss: 0.3366 - val_acc: 0.8558\n",
      "Epoch 28/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3859 - acc: 0.8510 - val_loss: 0.3216 - val_acc: 0.8746\n",
      "Epoch 29/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3577 - acc: 0.8604 - val_loss: 0.3208 - val_acc: 0.8840\n",
      "Epoch 30/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3699 - acc: 0.8541 - val_loss: 0.3796 - val_acc: 0.8621\n",
      "Epoch 31/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3509 - acc: 0.8573 - val_loss: 0.3723 - val_acc: 0.8370\n",
      "Epoch 32/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3792 - acc: 0.8580 - val_loss: 0.3285 - val_acc: 0.8777\n",
      "Epoch 33/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3900 - acc: 0.8439 - val_loss: 0.3142 - val_acc: 0.8809\n",
      "Epoch 34/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3602 - acc: 0.8518 - val_loss: 0.3240 - val_acc: 0.8871\n",
      "Epoch 35/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3757 - acc: 0.8447 - val_loss: 0.3396 - val_acc: 0.8715\n",
      "Epoch 36/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3629 - acc: 0.8541 - val_loss: 0.4089 - val_acc: 0.8182\n",
      "Epoch 37/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3609 - acc: 0.8627 - val_loss: 0.3304 - val_acc: 0.8621\n",
      "Epoch 38/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3592 - acc: 0.8565 - val_loss: 0.4179 - val_acc: 0.8150\n",
      "Epoch 39/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3709 - acc: 0.8494 - val_loss: 0.3178 - val_acc: 0.8934\n",
      "Epoch 40/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3672 - acc: 0.8525 - val_loss: 0.3246 - val_acc: 0.8871\n",
      "Epoch 41/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3786 - acc: 0.8549 - val_loss: 0.3592 - val_acc: 0.8401\n",
      "Epoch 42/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3534 - acc: 0.8667 - val_loss: 0.3258 - val_acc: 0.8652\n",
      "Epoch 43/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3615 - acc: 0.8525 - val_loss: 0.3743 - val_acc: 0.8339\n",
      "Epoch 44/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3665 - acc: 0.8557 - val_loss: 0.3077 - val_acc: 0.8777\n",
      "Epoch 45/250\n",
      "1275/1275 [==============================] - 0s 140us/step - loss: 0.3535 - acc: 0.8588 - val_loss: 0.3229 - val_acc: 0.8871\n",
      "Epoch 46/250\n",
      "1275/1275 [==============================] - 0s 109us/step - loss: 0.3648 - acc: 0.8573 - val_loss: 0.3197 - val_acc: 0.8903\n",
      "Epoch 47/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 113us/step - loss: 0.3699 - acc: 0.8573 - val_loss: 0.3279 - val_acc: 0.8903\n",
      "Epoch 48/250\n",
      "1275/1275 [==============================] - 0s 105us/step - loss: 0.3729 - acc: 0.8549 - val_loss: 0.3195 - val_acc: 0.8840\n",
      "Epoch 49/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3684 - acc: 0.8502 - val_loss: 0.3074 - val_acc: 0.8966\n",
      "Epoch 50/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3564 - acc: 0.8604 - val_loss: 0.3398 - val_acc: 0.8589\n",
      "Epoch 51/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3571 - acc: 0.8588 - val_loss: 0.4274 - val_acc: 0.8182\n",
      "Epoch 52/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3702 - acc: 0.8588 - val_loss: 0.3358 - val_acc: 0.8589\n",
      "Epoch 53/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3508 - acc: 0.8596 - val_loss: 0.3830 - val_acc: 0.8370\n",
      "Epoch 54/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3490 - acc: 0.8643 - val_loss: 0.4202 - val_acc: 0.8433\n",
      "Epoch 55/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3612 - acc: 0.8533 - val_loss: 0.3073 - val_acc: 0.8903\n",
      "Epoch 56/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3536 - acc: 0.8471 - val_loss: 0.3083 - val_acc: 0.8871\n",
      "Epoch 57/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3674 - acc: 0.8549 - val_loss: 0.3252 - val_acc: 0.8527\n",
      "Epoch 58/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3551 - acc: 0.8643 - val_loss: 0.3117 - val_acc: 0.8840\n",
      "Epoch 59/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3621 - acc: 0.8565 - val_loss: 0.3187 - val_acc: 0.8809\n",
      "Epoch 60/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3605 - acc: 0.8541 - val_loss: 0.3733 - val_acc: 0.8307\n",
      "Epoch 61/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3628 - acc: 0.8557 - val_loss: 0.3137 - val_acc: 0.8871\n",
      "Epoch 62/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3586 - acc: 0.8604 - val_loss: 0.3960 - val_acc: 0.8182\n",
      "Epoch 63/250\n",
      "1275/1275 [==============================] - 0s 141us/step - loss: 0.3451 - acc: 0.8612 - val_loss: 0.3647 - val_acc: 0.8401\n",
      "Epoch 64/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3582 - acc: 0.8502 - val_loss: 0.3594 - val_acc: 0.8589\n",
      "Epoch 65/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3558 - acc: 0.8627 - val_loss: 0.3534 - val_acc: 0.8464\n",
      "Epoch 66/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3587 - acc: 0.8643 - val_loss: 0.3947 - val_acc: 0.8213\n",
      "Epoch 67/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3602 - acc: 0.8604 - val_loss: 0.3067 - val_acc: 0.8840\n",
      "Epoch 68/250\n",
      "1275/1275 [==============================] - 0s 65us/step - loss: 0.3488 - acc: 0.8612 - val_loss: 0.3045 - val_acc: 0.8903\n",
      "Epoch 69/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3521 - acc: 0.8596 - val_loss: 0.3653 - val_acc: 0.8339\n",
      "Epoch 70/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3619 - acc: 0.8612 - val_loss: 0.3195 - val_acc: 0.8558\n",
      "Epoch 71/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.3585 - acc: 0.8627 - val_loss: 0.3080 - val_acc: 0.8903\n",
      "Epoch 72/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3576 - acc: 0.8612 - val_loss: 0.3211 - val_acc: 0.8621\n",
      "Epoch 73/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3573 - acc: 0.8588 - val_loss: 0.3067 - val_acc: 0.8809\n",
      "Epoch 74/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3646 - acc: 0.8525 - val_loss: 0.3061 - val_acc: 0.8871\n",
      "Epoch 75/250\n",
      "1275/1275 [==============================] - ETA: 0s - loss: 0.3743 - acc: 0.851 - 0s 82us/step - loss: 0.3478 - acc: 0.8659 - val_loss: 0.3167 - val_acc: 0.8715\n",
      "Epoch 76/250\n",
      "1275/1275 [==============================] - 0s 108us/step - loss: 0.3547 - acc: 0.8580 - val_loss: 0.3159 - val_acc: 0.8809\n",
      "Epoch 77/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3567 - acc: 0.8580 - val_loss: 0.3147 - val_acc: 0.8934\n",
      "Epoch 78/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3509 - acc: 0.8596 - val_loss: 0.3450 - val_acc: 0.8464\n",
      "Epoch 79/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3627 - acc: 0.8533 - val_loss: 0.3336 - val_acc: 0.8495\n",
      "Epoch 80/250\n",
      "1275/1275 [==============================] - 0s 97us/step - loss: 0.3565 - acc: 0.8565 - val_loss: 0.3110 - val_acc: 0.8871\n",
      "Epoch 81/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.3489 - acc: 0.8627 - val_loss: 0.3532 - val_acc: 0.8464\n",
      "Epoch 82/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3486 - acc: 0.8675 - val_loss: 0.3013 - val_acc: 0.8840\n",
      "Epoch 83/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3465 - acc: 0.8643 - val_loss: 0.3459 - val_acc: 0.8683\n",
      "Epoch 84/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3530 - acc: 0.8659 - val_loss: 0.3050 - val_acc: 0.8871\n",
      "Epoch 85/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3573 - acc: 0.8565 - val_loss: 0.3503 - val_acc: 0.8433\n",
      "Epoch 86/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3522 - acc: 0.8612 - val_loss: 0.3008 - val_acc: 0.9028\n",
      "Epoch 87/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3444 - acc: 0.8635 - val_loss: 0.3310 - val_acc: 0.8777\n",
      "Epoch 88/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3617 - acc: 0.8549 - val_loss: 0.3105 - val_acc: 0.8871\n",
      "Epoch 89/250\n",
      "1275/1275 [==============================] - 0s 99us/step - loss: 0.3414 - acc: 0.8596 - val_loss: 0.3284 - val_acc: 0.8777\n",
      "Epoch 90/250\n",
      "1275/1275 [==============================] - 0s 102us/step - loss: 0.3437 - acc: 0.8565 - val_loss: 0.3537 - val_acc: 0.8464\n",
      "Epoch 91/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3640 - acc: 0.8635 - val_loss: 0.3154 - val_acc: 0.8903\n",
      "Epoch 92/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3503 - acc: 0.8651 - val_loss: 0.3507 - val_acc: 0.8527\n",
      "Epoch 93/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3605 - acc: 0.8486 - val_loss: 0.3287 - val_acc: 0.8558\n",
      "Epoch 94/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3560 - acc: 0.8565 - val_loss: 0.3362 - val_acc: 0.8495\n",
      "Epoch 95/250\n",
      "1275/1275 [==============================] - 0s 108us/step - loss: 0.3502 - acc: 0.8643 - val_loss: 0.3468 - val_acc: 0.8809\n",
      "Epoch 96/250\n",
      "1275/1275 [==============================] - 0s 110us/step - loss: 0.3454 - acc: 0.8659 - val_loss: 0.3386 - val_acc: 0.8809\n",
      "Epoch 97/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3585 - acc: 0.8541 - val_loss: 0.4017 - val_acc: 0.8276\n",
      "Epoch 98/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.3611 - acc: 0.8518 - val_loss: 0.3071 - val_acc: 0.8840\n",
      "Epoch 99/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3470 - acc: 0.8612 - val_loss: 0.3231 - val_acc: 0.8746\n",
      "Epoch 100/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3472 - acc: 0.8635 - val_loss: 0.2986 - val_acc: 0.8934\n",
      "Epoch 101/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3445 - acc: 0.8682 - val_loss: 0.3337 - val_acc: 0.8840\n",
      "Epoch 102/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3539 - acc: 0.8541 - val_loss: 0.3120 - val_acc: 0.8683\n",
      "Epoch 103/250\n",
      "1275/1275 [==============================] - 0s 117us/step - loss: 0.3419 - acc: 0.8588 - val_loss: 0.2975 - val_acc: 0.8777\n",
      "Epoch 104/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3522 - acc: 0.8588 - val_loss: 0.3093 - val_acc: 0.8871\n",
      "Epoch 105/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3445 - acc: 0.8604 - val_loss: 0.3414 - val_acc: 0.8903\n",
      "Epoch 106/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3449 - acc: 0.8675 - val_loss: 0.3119 - val_acc: 0.8871\n",
      "Epoch 107/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.3546 - acc: 0.8643 - val_loss: 0.3796 - val_acc: 0.8621\n",
      "Epoch 108/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3531 - acc: 0.8620 - val_loss: 0.3834 - val_acc: 0.8307\n",
      "Epoch 109/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3400 - acc: 0.8588 - val_loss: 0.3891 - val_acc: 0.8527\n",
      "Epoch 110/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3508 - acc: 0.8565 - val_loss: 0.3232 - val_acc: 0.8777\n",
      "Epoch 111/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3438 - acc: 0.8627 - val_loss: 0.3167 - val_acc: 0.8934\n",
      "Epoch 112/250\n",
      "1275/1275 [==============================] - ETA: 0s - loss: 0.3700 - acc: 0.850 - 0s 79us/step - loss: 0.3464 - acc: 0.8643 - val_loss: 0.2966 - val_acc: 0.8871\n",
      "Epoch 113/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3529 - acc: 0.8588 - val_loss: 0.3097 - val_acc: 0.8934\n",
      "Epoch 114/250\n",
      "1275/1275 [==============================] - 0s 98us/step - loss: 0.3382 - acc: 0.8635 - val_loss: 0.3876 - val_acc: 0.8245\n",
      "Epoch 115/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.3429 - acc: 0.8612 - val_loss: 0.3103 - val_acc: 0.8934\n",
      "Epoch 116/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3400 - acc: 0.8643 - val_loss: 0.2960 - val_acc: 0.8903\n",
      "Epoch 117/250\n",
      "1275/1275 [==============================] - 0s 105us/step - loss: 0.3475 - acc: 0.8627 - val_loss: 0.2988 - val_acc: 0.8903\n",
      "Epoch 118/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3398 - acc: 0.8682 - val_loss: 0.3198 - val_acc: 0.8558\n",
      "Epoch 119/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3400 - acc: 0.8635 - val_loss: 0.2910 - val_acc: 0.8934\n",
      "Epoch 120/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3403 - acc: 0.8682 - val_loss: 0.3050 - val_acc: 0.8934\n",
      "Epoch 121/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3447 - acc: 0.8604 - val_loss: 0.2991 - val_acc: 0.8809\n",
      "Epoch 122/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3547 - acc: 0.8518 - val_loss: 0.3098 - val_acc: 0.8621\n",
      "Epoch 123/250\n",
      "1275/1275 [==============================] - 0s 121us/step - loss: 0.3428 - acc: 0.8580 - val_loss: 0.3468 - val_acc: 0.8903\n",
      "Epoch 124/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3424 - acc: 0.8667 - val_loss: 0.3059 - val_acc: 0.8903\n",
      "Epoch 125/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3447 - acc: 0.8682 - val_loss: 0.3643 - val_acc: 0.8370\n",
      "Epoch 126/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3533 - acc: 0.8573 - val_loss: 0.3112 - val_acc: 0.8621\n",
      "Epoch 127/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.3527 - acc: 0.8635 - val_loss: 0.4158 - val_acc: 0.8245\n",
      "Epoch 128/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3436 - acc: 0.8651 - val_loss: 0.2945 - val_acc: 0.8903\n",
      "Epoch 129/250\n",
      "1275/1275 [==============================] - 0s 66us/step - loss: 0.3428 - acc: 0.8580 - val_loss: 0.3140 - val_acc: 0.8652\n",
      "Epoch 130/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3441 - acc: 0.8635 - val_loss: 0.3110 - val_acc: 0.8903\n",
      "Epoch 131/250\n",
      "1275/1275 [==============================] - 0s 109us/step - loss: 0.3341 - acc: 0.8612 - val_loss: 0.3185 - val_acc: 0.8809\n",
      "Epoch 132/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3494 - acc: 0.8596 - val_loss: 0.3202 - val_acc: 0.8715\n",
      "Epoch 133/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3421 - acc: 0.8557 - val_loss: 0.3095 - val_acc: 0.8652\n",
      "Epoch 134/250\n",
      "1275/1275 [==============================] - 0s 109us/step - loss: 0.3391 - acc: 0.8769 - val_loss: 0.3072 - val_acc: 0.8840\n",
      "Epoch 135/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3522 - acc: 0.8620 - val_loss: 0.3173 - val_acc: 0.8777\n",
      "Epoch 136/250\n",
      "1275/1275 [==============================] - 0s 105us/step - loss: 0.3324 - acc: 0.8706 - val_loss: 0.2892 - val_acc: 0.8871\n",
      "Epoch 137/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3445 - acc: 0.8627 - val_loss: 0.4137 - val_acc: 0.8245\n",
      "Epoch 138/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3424 - acc: 0.8557 - val_loss: 0.2891 - val_acc: 0.8903\n",
      "Epoch 139/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3412 - acc: 0.8643 - val_loss: 0.3291 - val_acc: 0.8589\n",
      "Epoch 140/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3452 - acc: 0.8620 - val_loss: 0.2961 - val_acc: 0.8809\n",
      "Epoch 141/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3421 - acc: 0.8612 - val_loss: 0.3242 - val_acc: 0.8934\n",
      "Epoch 142/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3377 - acc: 0.8659 - val_loss: 0.3298 - val_acc: 0.8840\n",
      "Epoch 143/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3471 - acc: 0.8706 - val_loss: 0.2966 - val_acc: 0.8715\n",
      "Epoch 144/250\n",
      "1275/1275 [==============================] - 0s 119us/step - loss: 0.3472 - acc: 0.8667 - val_loss: 0.2953 - val_acc: 0.8903\n",
      "Epoch 145/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3401 - acc: 0.8667 - val_loss: 0.3228 - val_acc: 0.8683\n",
      "Epoch 146/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.3316 - acc: 0.8643 - val_loss: 0.3286 - val_acc: 0.8589\n",
      "Epoch 147/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3331 - acc: 0.8682 - val_loss: 0.3069 - val_acc: 0.8746\n",
      "Epoch 148/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3499 - acc: 0.8549 - val_loss: 0.2934 - val_acc: 0.8715\n",
      "Epoch 149/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3383 - acc: 0.8627 - val_loss: 0.3161 - val_acc: 0.8903\n",
      "Epoch 150/250\n",
      "1275/1275 [==============================] - 0s 125us/step - loss: 0.3330 - acc: 0.8667 - val_loss: 0.3129 - val_acc: 0.8777\n",
      "Epoch 151/250\n",
      "1275/1275 [==============================] - 0s 102us/step - loss: 0.3497 - acc: 0.8573 - val_loss: 0.3025 - val_acc: 0.8683\n",
      "Epoch 152/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3408 - acc: 0.8627 - val_loss: 0.2922 - val_acc: 0.8934\n",
      "Epoch 153/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3362 - acc: 0.8612 - val_loss: 0.2958 - val_acc: 0.8903\n",
      "Epoch 154/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3415 - acc: 0.8682 - val_loss: 0.3104 - val_acc: 0.8652\n",
      "Epoch 155/250\n",
      "1275/1275 [==============================] - 0s 93us/step - loss: 0.3344 - acc: 0.8698 - val_loss: 0.3324 - val_acc: 0.8527\n",
      "Epoch 156/250\n",
      "1275/1275 [==============================] - 0s 112us/step - loss: 0.3386 - acc: 0.8620 - val_loss: 0.3318 - val_acc: 0.8558\n",
      "Epoch 157/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3301 - acc: 0.8667 - val_loss: 0.3412 - val_acc: 0.8746\n",
      "Epoch 158/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3312 - acc: 0.8612 - val_loss: 0.3428 - val_acc: 0.8840\n",
      "Epoch 159/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3360 - acc: 0.8698 - val_loss: 0.2998 - val_acc: 0.8746\n",
      "Epoch 160/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3357 - acc: 0.8690 - val_loss: 0.3097 - val_acc: 0.8683\n",
      "Epoch 161/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3350 - acc: 0.8651 - val_loss: 0.3615 - val_acc: 0.8433\n",
      "Epoch 162/250\n",
      "1275/1275 [==============================] - 0s 95us/step - loss: 0.3333 - acc: 0.8635 - val_loss: 0.2924 - val_acc: 0.8934\n",
      "Epoch 163/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3343 - acc: 0.8659 - val_loss: 0.3148 - val_acc: 0.8934\n",
      "Epoch 164/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3384 - acc: 0.8643 - val_loss: 0.3449 - val_acc: 0.8558\n",
      "Epoch 165/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3506 - acc: 0.8588 - val_loss: 0.3092 - val_acc: 0.8683\n",
      "Epoch 166/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3427 - acc: 0.8580 - val_loss: 0.3127 - val_acc: 0.8715\n",
      "Epoch 167/250\n",
      "1275/1275 [==============================] - ETA: 0s - loss: 0.3611 - acc: 0.861 - 0s 70us/step - loss: 0.3473 - acc: 0.8682 - val_loss: 0.3172 - val_acc: 0.8777\n",
      "Epoch 168/250\n",
      "1275/1275 [==============================] - 0s 110us/step - loss: 0.3347 - acc: 0.8643 - val_loss: 0.3325 - val_acc: 0.8589\n",
      "Epoch 169/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3329 - acc: 0.8675 - val_loss: 0.3034 - val_acc: 0.8903\n",
      "Epoch 170/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3296 - acc: 0.8737 - val_loss: 0.3066 - val_acc: 0.8903\n",
      "Epoch 171/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3339 - acc: 0.8706 - val_loss: 0.2877 - val_acc: 0.8809\n",
      "Epoch 172/250\n",
      "1275/1275 [==============================] - 0s 116us/step - loss: 0.3347 - acc: 0.8612 - val_loss: 0.3231 - val_acc: 0.8652\n",
      "Epoch 173/250\n",
      "1275/1275 [==============================] - 0s 102us/step - loss: 0.3371 - acc: 0.8651 - val_loss: 0.2836 - val_acc: 0.8840\n",
      "Epoch 174/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.3392 - acc: 0.8643 - val_loss: 0.2832 - val_acc: 0.8903\n",
      "Epoch 175/250\n",
      "1275/1275 [==============================] - 0s 70us/step - loss: 0.3269 - acc: 0.8714 - val_loss: 0.2909 - val_acc: 0.8871\n",
      "Epoch 176/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3324 - acc: 0.8659 - val_loss: 0.3072 - val_acc: 0.8871\n",
      "Epoch 177/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3361 - acc: 0.8557 - val_loss: 0.3051 - val_acc: 0.8840\n",
      "Epoch 178/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3360 - acc: 0.8620 - val_loss: 0.2884 - val_acc: 0.8934\n",
      "Epoch 179/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3277 - acc: 0.8690 - val_loss: 0.3494 - val_acc: 0.8558\n",
      "Epoch 180/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3368 - acc: 0.8635 - val_loss: 0.3166 - val_acc: 0.8621\n",
      "Epoch 181/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3360 - acc: 0.8675 - val_loss: 0.3186 - val_acc: 0.8809\n",
      "Epoch 182/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3370 - acc: 0.8651 - val_loss: 0.3070 - val_acc: 0.8683\n",
      "Epoch 183/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3376 - acc: 0.8627 - val_loss: 0.3211 - val_acc: 0.8746\n",
      "Epoch 184/250\n",
      "1275/1275 [==============================] - 0s 102us/step - loss: 0.3371 - acc: 0.8722 - val_loss: 0.2916 - val_acc: 0.8934\n",
      "Epoch 185/250\n",
      "1275/1275 [==============================] - 0s 117us/step - loss: 0.3265 - acc: 0.8737 - val_loss: 0.3041 - val_acc: 0.8934\n",
      "Epoch 186/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3232 - acc: 0.8643 - val_loss: 0.2835 - val_acc: 0.8903\n",
      "Epoch 187/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3346 - acc: 0.8690 - val_loss: 0.3221 - val_acc: 0.8589\n",
      "Epoch 188/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3368 - acc: 0.8659 - val_loss: 0.3097 - val_acc: 0.8621\n",
      "Epoch 189/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3393 - acc: 0.8612 - val_loss: 0.2920 - val_acc: 0.8903\n",
      "Epoch 190/250\n",
      "1275/1275 [==============================] - 0s 102us/step - loss: 0.3448 - acc: 0.8588 - val_loss: 0.3009 - val_acc: 0.8683\n",
      "Epoch 191/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3302 - acc: 0.8690 - val_loss: 0.3104 - val_acc: 0.8966\n",
      "Epoch 192/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3257 - acc: 0.8651 - val_loss: 0.3181 - val_acc: 0.8652\n",
      "Epoch 193/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3302 - acc: 0.8659 - val_loss: 0.2900 - val_acc: 0.8903\n",
      "Epoch 194/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3513 - acc: 0.8573 - val_loss: 0.3409 - val_acc: 0.8495\n",
      "Epoch 195/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.3301 - acc: 0.8729 - val_loss: 0.2993 - val_acc: 0.8777\n",
      "Epoch 196/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3310 - acc: 0.8729 - val_loss: 0.3487 - val_acc: 0.8527\n",
      "Epoch 197/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3252 - acc: 0.8682 - val_loss: 0.3164 - val_acc: 0.8746\n",
      "Epoch 198/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3263 - acc: 0.8667 - val_loss: 0.3119 - val_acc: 0.8809\n",
      "Epoch 199/250\n",
      "1275/1275 [==============================] - 0s 105us/step - loss: 0.3327 - acc: 0.8659 - val_loss: 0.3162 - val_acc: 0.8871\n",
      "Epoch 200/250\n",
      "1275/1275 [==============================] - 0s 116us/step - loss: 0.3205 - acc: 0.8745 - val_loss: 0.3167 - val_acc: 0.8652\n",
      "Epoch 201/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3307 - acc: 0.8573 - val_loss: 0.3008 - val_acc: 0.8840\n",
      "Epoch 202/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3387 - acc: 0.8706 - val_loss: 0.2917 - val_acc: 0.8809\n",
      "Epoch 203/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.3337 - acc: 0.8635 - val_loss: 0.3252 - val_acc: 0.8652\n",
      "Epoch 204/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3281 - acc: 0.8714 - val_loss: 0.3202 - val_acc: 0.8621\n",
      "Epoch 205/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3320 - acc: 0.8706 - val_loss: 0.2861 - val_acc: 0.8934\n",
      "Epoch 206/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3445 - acc: 0.8612 - val_loss: 0.2947 - val_acc: 0.8997\n",
      "Epoch 207/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3363 - acc: 0.8690 - val_loss: 0.2872 - val_acc: 0.8840\n",
      "Epoch 208/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.3347 - acc: 0.8682 - val_loss: 0.2816 - val_acc: 0.8966\n",
      "Epoch 209/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3308 - acc: 0.8635 - val_loss: 0.2925 - val_acc: 0.8809\n",
      "Epoch 210/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3318 - acc: 0.8659 - val_loss: 0.3192 - val_acc: 0.8715\n",
      "Epoch 211/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3261 - acc: 0.8714 - val_loss: 0.3242 - val_acc: 0.8715\n",
      "Epoch 212/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3385 - acc: 0.8620 - val_loss: 0.3024 - val_acc: 0.8966\n",
      "Epoch 213/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.3414 - acc: 0.8675 - val_loss: 0.2970 - val_acc: 0.8777\n",
      "Epoch 214/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.3321 - acc: 0.8675 - val_loss: 0.3186 - val_acc: 0.8903\n",
      "Epoch 215/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3375 - acc: 0.8612 - val_loss: 0.2800 - val_acc: 0.8934\n",
      "Epoch 216/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3198 - acc: 0.8737 - val_loss: 0.2896 - val_acc: 0.8966\n",
      "Epoch 217/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3347 - acc: 0.8667 - val_loss: 0.2881 - val_acc: 0.8934\n",
      "Epoch 218/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3179 - acc: 0.8722 - val_loss: 0.3313 - val_acc: 0.8903\n",
      "Epoch 219/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3406 - acc: 0.8588 - val_loss: 0.2853 - val_acc: 0.8871\n",
      "Epoch 220/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3360 - acc: 0.8604 - val_loss: 0.2817 - val_acc: 0.8871\n",
      "Epoch 221/250\n",
      "1275/1275 [==============================] - 0s 119us/step - loss: 0.3278 - acc: 0.8667 - val_loss: 0.3055 - val_acc: 0.8871\n",
      "Epoch 222/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3289 - acc: 0.8737 - val_loss: 0.2912 - val_acc: 0.8966\n",
      "Epoch 223/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3287 - acc: 0.8698 - val_loss: 0.3525 - val_acc: 0.8715\n",
      "Epoch 224/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3288 - acc: 0.8714 - val_loss: 0.2785 - val_acc: 0.8871\n",
      "Epoch 225/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3325 - acc: 0.8714 - val_loss: 0.3132 - val_acc: 0.8652\n",
      "Epoch 226/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3281 - acc: 0.8643 - val_loss: 0.3830 - val_acc: 0.8840\n",
      "Epoch 227/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.3324 - acc: 0.8635 - val_loss: 0.2823 - val_acc: 0.8966\n",
      "Epoch 228/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3323 - acc: 0.8635 - val_loss: 0.2999 - val_acc: 0.8997\n",
      "Epoch 229/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3428 - acc: 0.8675 - val_loss: 0.3571 - val_acc: 0.8401\n",
      "Epoch 230/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3320 - acc: 0.8714 - val_loss: 0.2940 - val_acc: 0.8809\n",
      "Epoch 231/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.3208 - acc: 0.8737 - val_loss: 0.3249 - val_acc: 0.8621\n",
      "Epoch 232/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3278 - acc: 0.8651 - val_loss: 0.2957 - val_acc: 0.8966\n",
      "Epoch 233/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3285 - acc: 0.8675 - val_loss: 0.3209 - val_acc: 0.8966\n",
      "Epoch 234/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3291 - acc: 0.8682 - val_loss: 0.3188 - val_acc: 0.8621\n",
      "Epoch 235/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.3228 - acc: 0.8706 - val_loss: 0.3291 - val_acc: 0.8652\n",
      "Epoch 236/250\n",
      "1275/1275 [==============================] - 0s 98us/step - loss: 0.3275 - acc: 0.8667 - val_loss: 0.2877 - val_acc: 0.8903\n",
      "Epoch 237/250\n",
      "1275/1275 [==============================] - 0s 155us/step - loss: 0.3276 - acc: 0.8659 - val_loss: 0.3010 - val_acc: 0.8871\n",
      "Epoch 238/250\n",
      "1275/1275 [==============================] - 0s 110us/step - loss: 0.3266 - acc: 0.8698 - val_loss: 0.2820 - val_acc: 0.8777\n",
      "Epoch 239/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3213 - acc: 0.8698 - val_loss: 0.3081 - val_acc: 0.8683\n",
      "Epoch 240/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3299 - acc: 0.8698 - val_loss: 0.3481 - val_acc: 0.8527\n",
      "Epoch 241/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3261 - acc: 0.8745 - val_loss: 0.3168 - val_acc: 0.8683\n",
      "Epoch 242/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3251 - acc: 0.8627 - val_loss: 0.3120 - val_acc: 0.8997\n",
      "Epoch 243/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3373 - acc: 0.8690 - val_loss: 0.2958 - val_acc: 0.8777\n",
      "Epoch 244/250\n",
      "1275/1275 [==============================] - 0s 109us/step - loss: 0.3215 - acc: 0.8714 - val_loss: 0.3053 - val_acc: 0.8715\n",
      "Epoch 245/250\n",
      "1275/1275 [==============================] - 0s 132us/step - loss: 0.3336 - acc: 0.8651 - val_loss: 0.2918 - val_acc: 0.8966\n",
      "Epoch 246/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3315 - acc: 0.8635 - val_loss: 0.3119 - val_acc: 0.8777\n",
      "Epoch 247/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3328 - acc: 0.8706 - val_loss: 0.3058 - val_acc: 0.8715\n",
      "Epoch 248/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3288 - acc: 0.8627 - val_loss: 0.3571 - val_acc: 0.8495\n",
      "Epoch 249/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3262 - acc: 0.8667 - val_loss: 0.2982 - val_acc: 0.8715\n",
      "Epoch 250/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3282 - acc: 0.8745 - val_loss: 0.2991 - val_acc: 0.8903\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.050000\n",
      "\n",
      "Train on 1275 samples, validate on 319 samples\n",
      "Epoch 1/250\n",
      "1275/1275 [==============================] - 2s 1ms/step - loss: 0.3550 - acc: 0.8620 - val_loss: 0.2852 - val_acc: 0.8997\n",
      "Epoch 2/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3471 - acc: 0.8659 - val_loss: 0.3387 - val_acc: 0.8652\n",
      "Epoch 3/250\n",
      "1275/1275 [==============================] - 0s 125us/step - loss: 0.3414 - acc: 0.8580 - val_loss: 0.3531 - val_acc: 0.8495\n",
      "Epoch 4/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3536 - acc: 0.8620 - val_loss: 0.4218 - val_acc: 0.8307\n",
      "Epoch 5/250\n",
      "1275/1275 [==============================] - 0s 123us/step - loss: 0.3608 - acc: 0.8541 - val_loss: 0.5612 - val_acc: 0.7931\n",
      "Epoch 6/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3492 - acc: 0.8659 - val_loss: 0.3106 - val_acc: 0.8746\n",
      "Epoch 7/250\n",
      "1275/1275 [==============================] - 0s 123us/step - loss: 0.3393 - acc: 0.8682 - val_loss: 0.3336 - val_acc: 0.8527\n",
      "Epoch 8/250\n",
      "1275/1275 [==============================] - 0s 140us/step - loss: 0.3476 - acc: 0.8518 - val_loss: 0.3175 - val_acc: 0.8746\n",
      "Epoch 9/250\n",
      "1275/1275 [==============================] - 0s 123us/step - loss: 0.3490 - acc: 0.8667 - val_loss: 0.3004 - val_acc: 0.8683\n",
      "Epoch 10/250\n",
      "1275/1275 [==============================] - 0s 201us/step - loss: 0.3407 - acc: 0.8675 - val_loss: 0.3646 - val_acc: 0.8464\n",
      "Epoch 11/250\n",
      "1275/1275 [==============================] - 0s 161us/step - loss: 0.3455 - acc: 0.8588 - val_loss: 0.3192 - val_acc: 0.8589\n",
      "Epoch 12/250\n",
      "1275/1275 [==============================] - 0s 139us/step - loss: 0.3338 - acc: 0.8659 - val_loss: 0.3480 - val_acc: 0.8652\n",
      "Epoch 13/250\n",
      "1275/1275 [==============================] - 0s 173us/step - loss: 0.3384 - acc: 0.8604 - val_loss: 0.3117 - val_acc: 0.8589\n",
      "Epoch 14/250\n",
      "1275/1275 [==============================] - 0s 209us/step - loss: 0.3437 - acc: 0.8627 - val_loss: 0.4018 - val_acc: 0.8589\n",
      "Epoch 15/250\n",
      "1275/1275 [==============================] - ETA: 0s - loss: 0.3488 - acc: 0.869 - 0s 149us/step - loss: 0.3462 - acc: 0.8682 - val_loss: 0.3094 - val_acc: 0.8715\n",
      "Epoch 16/250\n",
      "1275/1275 [==============================] - 0s 132us/step - loss: 0.3434 - acc: 0.8667 - val_loss: 0.2955 - val_acc: 0.8903\n",
      "Epoch 17/250\n",
      "1275/1275 [==============================] - 0s 123us/step - loss: 0.3368 - acc: 0.8675 - val_loss: 0.3078 - val_acc: 0.8746\n",
      "Epoch 18/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3463 - acc: 0.8557 - val_loss: 0.2780 - val_acc: 0.8934\n",
      "Epoch 19/250\n",
      "1275/1275 [==============================] - ETA: 0s - loss: 0.3528 - acc: 0.853 - 0s 145us/step - loss: 0.3421 - acc: 0.8604 - val_loss: 0.3095 - val_acc: 0.8934\n",
      "Epoch 20/250\n",
      "1275/1275 [==============================] - 0s 132us/step - loss: 0.3565 - acc: 0.8541 - val_loss: 0.2935 - val_acc: 0.8903\n",
      "Epoch 21/250\n",
      "1275/1275 [==============================] - 0s 166us/step - loss: 0.3286 - acc: 0.8776 - val_loss: 0.2772 - val_acc: 0.8934\n",
      "Epoch 22/250\n",
      "1275/1275 [==============================] - 0s 132us/step - loss: 0.3404 - acc: 0.8667 - val_loss: 0.3083 - val_acc: 0.8683\n",
      "Epoch 23/250\n",
      "1275/1275 [==============================] - 0s 131us/step - loss: 0.3382 - acc: 0.8643 - val_loss: 0.3043 - val_acc: 0.8871\n",
      "Epoch 24/250\n",
      "1275/1275 [==============================] - 0s 121us/step - loss: 0.3364 - acc: 0.8706 - val_loss: 0.3015 - val_acc: 0.8809\n",
      "Epoch 25/250\n",
      "1275/1275 [==============================] - 0s 155us/step - loss: 0.3485 - acc: 0.8690 - val_loss: 0.2988 - val_acc: 0.8777\n",
      "Epoch 26/250\n",
      "1275/1275 [==============================] - 0s 143us/step - loss: 0.3444 - acc: 0.8596 - val_loss: 0.2987 - val_acc: 0.8777\n",
      "Epoch 27/250\n",
      "1275/1275 [==============================] - 0s 124us/step - loss: 0.3287 - acc: 0.8643 - val_loss: 0.3460 - val_acc: 0.8871\n",
      "Epoch 28/250\n",
      "1275/1275 [==============================] - 0s 140us/step - loss: 0.3446 - acc: 0.8643 - val_loss: 0.2982 - val_acc: 0.8746\n",
      "Epoch 29/250\n",
      "1275/1275 [==============================] - 0s 126us/step - loss: 0.3405 - acc: 0.8675 - val_loss: 0.2778 - val_acc: 0.8840\n",
      "Epoch 30/250\n",
      "1275/1275 [==============================] - 0s 160us/step - loss: 0.3404 - acc: 0.8612 - val_loss: 0.3404 - val_acc: 0.8777\n",
      "Epoch 31/250\n",
      "1275/1275 [==============================] - 0s 121us/step - loss: 0.3290 - acc: 0.8675 - val_loss: 0.3195 - val_acc: 0.8589\n",
      "Epoch 32/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3515 - acc: 0.8565 - val_loss: 0.2966 - val_acc: 0.8966\n",
      "Epoch 33/250\n",
      "1275/1275 [==============================] - 0s 120us/step - loss: 0.3367 - acc: 0.8690 - val_loss: 0.2994 - val_acc: 0.8683\n",
      "Epoch 34/250\n",
      "1275/1275 [==============================] - 0s 122us/step - loss: 0.3408 - acc: 0.8675 - val_loss: 0.3363 - val_acc: 0.8840\n",
      "Epoch 35/250\n",
      "1275/1275 [==============================] - 0s 139us/step - loss: 0.3245 - acc: 0.8722 - val_loss: 0.3376 - val_acc: 0.8589\n",
      "Epoch 36/250\n",
      "1275/1275 [==============================] - 0s 142us/step - loss: 0.3276 - acc: 0.8706 - val_loss: 0.3308 - val_acc: 0.8589\n",
      "Epoch 37/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.3330 - acc: 0.8682 - val_loss: 0.3153 - val_acc: 0.8871\n",
      "Epoch 38/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3404 - acc: 0.8690 - val_loss: 0.3801 - val_acc: 0.8715\n",
      "Epoch 39/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3557 - acc: 0.8588 - val_loss: 0.3258 - val_acc: 0.8683\n",
      "Epoch 40/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3467 - acc: 0.8612 - val_loss: 0.3026 - val_acc: 0.8777\n",
      "Epoch 41/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3295 - acc: 0.8761 - val_loss: 0.3195 - val_acc: 0.8903\n",
      "Epoch 42/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3473 - acc: 0.8690 - val_loss: 0.2859 - val_acc: 0.8809\n",
      "Epoch 43/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3250 - acc: 0.8620 - val_loss: 0.3293 - val_acc: 0.8746\n",
      "Epoch 44/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3243 - acc: 0.8729 - val_loss: 0.3134 - val_acc: 0.8777\n",
      "Epoch 45/250\n",
      "1275/1275 [==============================] - 0s 130us/step - loss: 0.3330 - acc: 0.8635 - val_loss: 0.2731 - val_acc: 0.8903\n",
      "Epoch 46/250\n",
      "1275/1275 [==============================] - 0s 98us/step - loss: 0.3290 - acc: 0.8698 - val_loss: 0.3211 - val_acc: 0.8777\n",
      "Epoch 47/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3520 - acc: 0.8596 - val_loss: 0.2951 - val_acc: 0.8777\n",
      "Epoch 48/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3257 - acc: 0.8745 - val_loss: 0.2927 - val_acc: 0.8809\n",
      "Epoch 49/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3315 - acc: 0.8714 - val_loss: 0.2730 - val_acc: 0.8871\n",
      "Epoch 50/250\n",
      "1275/1275 [==============================] - 0s 70us/step - loss: 0.3330 - acc: 0.8651 - val_loss: 0.3082 - val_acc: 0.8840\n",
      "Epoch 51/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3475 - acc: 0.8604 - val_loss: 0.2882 - val_acc: 0.8809\n",
      "Epoch 52/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3238 - acc: 0.8651 - val_loss: 0.3074 - val_acc: 0.9028\n",
      "Epoch 53/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3398 - acc: 0.8588 - val_loss: 0.2924 - val_acc: 0.8840\n",
      "Epoch 54/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3403 - acc: 0.8682 - val_loss: 0.2936 - val_acc: 0.8966\n",
      "Epoch 55/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3466 - acc: 0.8627 - val_loss: 0.2706 - val_acc: 0.8966\n",
      "Epoch 56/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3296 - acc: 0.8722 - val_loss: 0.2787 - val_acc: 0.8966\n",
      "Epoch 57/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3188 - acc: 0.8682 - val_loss: 0.3077 - val_acc: 0.8966\n",
      "Epoch 58/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3133 - acc: 0.8706 - val_loss: 0.2725 - val_acc: 0.8997\n",
      "Epoch 59/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3291 - acc: 0.8643 - val_loss: 0.3747 - val_acc: 0.8339\n",
      "Epoch 60/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3342 - acc: 0.8745 - val_loss: 0.3057 - val_acc: 0.8715\n",
      "Epoch 61/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3522 - acc: 0.8557 - val_loss: 0.2932 - val_acc: 0.8715\n",
      "Epoch 62/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3224 - acc: 0.8729 - val_loss: 0.3095 - val_acc: 0.8903\n",
      "Epoch 63/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3344 - acc: 0.8682 - val_loss: 0.2772 - val_acc: 0.8840\n",
      "Epoch 64/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3294 - acc: 0.8690 - val_loss: 0.3215 - val_acc: 0.8715\n",
      "Epoch 65/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3377 - acc: 0.8635 - val_loss: 0.2928 - val_acc: 0.8934\n",
      "Epoch 66/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3175 - acc: 0.8675 - val_loss: 0.2915 - val_acc: 0.8903\n",
      "Epoch 67/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3212 - acc: 0.8675 - val_loss: 0.2757 - val_acc: 0.8840\n",
      "Epoch 68/250\n",
      "1275/1275 [==============================] - 0s 109us/step - loss: 0.3181 - acc: 0.8753 - val_loss: 0.4505 - val_acc: 0.8056\n",
      "Epoch 69/250\n",
      "1275/1275 [==============================] - 0s 109us/step - loss: 0.3260 - acc: 0.8659 - val_loss: 0.2866 - val_acc: 0.8871\n",
      "Epoch 70/250\n",
      "1275/1275 [==============================] - 0s 109us/step - loss: 0.3275 - acc: 0.8761 - val_loss: 0.3392 - val_acc: 0.8527\n",
      "Epoch 71/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3330 - acc: 0.8729 - val_loss: 0.2987 - val_acc: 0.8997\n",
      "Epoch 72/250\n",
      "1275/1275 [==============================] - 0s 137us/step - loss: 0.3376 - acc: 0.8651 - val_loss: 0.2944 - val_acc: 0.8840\n",
      "Epoch 73/250\n",
      "1275/1275 [==============================] - 0s 126us/step - loss: 0.3278 - acc: 0.8667 - val_loss: 0.2923 - val_acc: 0.9028\n",
      "Epoch 74/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3236 - acc: 0.8722 - val_loss: 0.2776 - val_acc: 0.9028\n",
      "Epoch 75/250\n",
      "1275/1275 [==============================] - 0s 106us/step - loss: 0.3257 - acc: 0.8690 - val_loss: 0.2859 - val_acc: 0.8966\n",
      "Epoch 76/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.3215 - acc: 0.8698 - val_loss: 0.2892 - val_acc: 0.8871\n",
      "Epoch 77/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3344 - acc: 0.8588 - val_loss: 0.3285 - val_acc: 0.8652\n",
      "Epoch 78/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3239 - acc: 0.8753 - val_loss: 0.3005 - val_acc: 0.8966\n",
      "Epoch 79/250\n",
      "1275/1275 [==============================] - 0s 159us/step - loss: 0.3284 - acc: 0.8769 - val_loss: 0.2730 - val_acc: 0.8809\n",
      "Epoch 80/250\n",
      "1275/1275 [==============================] - 0s 137us/step - loss: 0.3349 - acc: 0.8675 - val_loss: 0.3029 - val_acc: 0.8903\n",
      "Epoch 81/250\n",
      "1275/1275 [==============================] - 0s 123us/step - loss: 0.3293 - acc: 0.8690 - val_loss: 0.2808 - val_acc: 0.8934\n",
      "Epoch 82/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3184 - acc: 0.8776 - val_loss: 0.2730 - val_acc: 0.8840\n",
      "Epoch 83/250\n",
      "1275/1275 [==============================] - ETA: 0s - loss: 0.3277 - acc: 0.878 - 0s 114us/step - loss: 0.3386 - acc: 0.8769 - val_loss: 0.3149 - val_acc: 0.8840\n",
      "Epoch 84/250\n",
      "1275/1275 [==============================] - ETA: 0s - loss: 0.3337 - acc: 0.865 - 0s 113us/step - loss: 0.3348 - acc: 0.8651 - val_loss: 0.3417 - val_acc: 0.8621\n",
      "Epoch 85/250\n",
      "1275/1275 [==============================] - 0s 156us/step - loss: 0.3219 - acc: 0.8682 - val_loss: 0.2938 - val_acc: 0.8997\n",
      "Epoch 86/250\n",
      "1275/1275 [==============================] - 0s 132us/step - loss: 0.3232 - acc: 0.8675 - val_loss: 0.3694 - val_acc: 0.8339\n",
      "Epoch 87/250\n",
      "1275/1275 [==============================] - 0s 93us/step - loss: 0.3322 - acc: 0.8643 - val_loss: 0.2845 - val_acc: 0.9060\n",
      "Epoch 88/250\n",
      "1275/1275 [==============================] - 0s 95us/step - loss: 0.3298 - acc: 0.8682 - val_loss: 0.2854 - val_acc: 0.8777\n",
      "Epoch 89/250\n",
      "1275/1275 [==============================] - 0s 116us/step - loss: 0.3216 - acc: 0.8706 - val_loss: 0.2890 - val_acc: 0.8871\n",
      "Epoch 90/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3314 - acc: 0.8745 - val_loss: 0.2792 - val_acc: 0.8966\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.070000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1275 samples, validate on 319 samples\n",
      "Epoch 1/250\n",
      "1275/1275 [==============================] - 2s 1ms/step - loss: 0.3294 - acc: 0.8690 - val_loss: 0.4263 - val_acc: 0.8245\n",
      "Epoch 2/250\n",
      "1275/1275 [==============================] - 0s 67us/step - loss: 0.3824 - acc: 0.8447 - val_loss: 0.3011 - val_acc: 0.8683\n",
      "Epoch 3/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3503 - acc: 0.8635 - val_loss: 0.2792 - val_acc: 0.8621\n",
      "Epoch 4/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3527 - acc: 0.8588 - val_loss: 0.3133 - val_acc: 0.8809\n",
      "Epoch 5/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3468 - acc: 0.8588 - val_loss: 0.3539 - val_acc: 0.8746\n",
      "Epoch 6/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3419 - acc: 0.8667 - val_loss: 0.2898 - val_acc: 0.8715\n",
      "Epoch 7/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3380 - acc: 0.8651 - val_loss: 0.2814 - val_acc: 0.8715\n",
      "Epoch 8/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3555 - acc: 0.8518 - val_loss: 0.2892 - val_acc: 0.8746\n",
      "Epoch 9/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3345 - acc: 0.8659 - val_loss: 0.2684 - val_acc: 0.8934\n",
      "Epoch 10/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3331 - acc: 0.8690 - val_loss: 0.3025 - val_acc: 0.8966\n",
      "Epoch 11/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3389 - acc: 0.8627 - val_loss: 0.3043 - val_acc: 0.8934\n",
      "Epoch 12/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3237 - acc: 0.8792 - val_loss: 0.3536 - val_acc: 0.8840\n",
      "Epoch 13/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3385 - acc: 0.8737 - val_loss: 0.3015 - val_acc: 0.8746\n",
      "Epoch 14/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3324 - acc: 0.8682 - val_loss: 0.2704 - val_acc: 0.8997\n",
      "Epoch 15/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3381 - acc: 0.8667 - val_loss: 0.2760 - val_acc: 0.8746\n",
      "Epoch 16/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3392 - acc: 0.8667 - val_loss: 0.2783 - val_acc: 0.8997\n",
      "Epoch 17/250\n",
      "1275/1275 [==============================] - 0s 134us/step - loss: 0.3407 - acc: 0.8635 - val_loss: 0.3062 - val_acc: 0.8871\n",
      "Epoch 18/250\n",
      "1275/1275 [==============================] - 0s 115us/step - loss: 0.3491 - acc: 0.8580 - val_loss: 0.3406 - val_acc: 0.8527\n",
      "Epoch 19/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3380 - acc: 0.8698 - val_loss: 0.2823 - val_acc: 0.8966\n",
      "Epoch 20/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3346 - acc: 0.8651 - val_loss: 0.3230 - val_acc: 0.8589\n",
      "Epoch 21/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3281 - acc: 0.8620 - val_loss: 0.2942 - val_acc: 0.8715\n",
      "Epoch 22/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3151 - acc: 0.8753 - val_loss: 0.3031 - val_acc: 0.8997\n",
      "Epoch 23/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3421 - acc: 0.8651 - val_loss: 0.2779 - val_acc: 0.8840\n",
      "Epoch 24/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3313 - acc: 0.8667 - val_loss: 0.2682 - val_acc: 0.8934\n",
      "Epoch 25/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3507 - acc: 0.8565 - val_loss: 0.3095 - val_acc: 0.8840\n",
      "Epoch 26/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3248 - acc: 0.8698 - val_loss: 0.2979 - val_acc: 0.9028\n",
      "Epoch 27/250\n",
      "1275/1275 [==============================] - 0s 106us/step - loss: 0.3263 - acc: 0.8682 - val_loss: 0.2847 - val_acc: 0.9028\n",
      "Epoch 28/250\n",
      "1275/1275 [==============================] - 0s 161us/step - loss: 0.3361 - acc: 0.8698 - val_loss: 0.2851 - val_acc: 0.8934\n",
      "Epoch 29/250\n",
      "1275/1275 [==============================] - 0s 93us/step - loss: 0.3436 - acc: 0.8596 - val_loss: 0.2977 - val_acc: 0.8934\n",
      "Epoch 30/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3328 - acc: 0.8682 - val_loss: 0.3068 - val_acc: 0.8777\n",
      "Epoch 31/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3207 - acc: 0.8714 - val_loss: 0.2868 - val_acc: 0.8840\n",
      "Epoch 32/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3582 - acc: 0.8588 - val_loss: 0.3904 - val_acc: 0.8245\n",
      "Epoch 33/250\n",
      "1275/1275 [==============================] - 0s 119us/step - loss: 0.3446 - acc: 0.8612 - val_loss: 0.2886 - val_acc: 0.8997\n",
      "Epoch 34/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3457 - acc: 0.8612 - val_loss: 0.3434 - val_acc: 0.8464\n",
      "Epoch 35/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3302 - acc: 0.8635 - val_loss: 0.2693 - val_acc: 0.8934\n",
      "Epoch 36/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3300 - acc: 0.8612 - val_loss: 0.3150 - val_acc: 0.8746\n",
      "Epoch 37/250\n",
      "1275/1275 [==============================] - 0s 132us/step - loss: 0.3202 - acc: 0.8698 - val_loss: 0.2786 - val_acc: 0.8903\n",
      "Epoch 38/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3245 - acc: 0.8635 - val_loss: 0.2888 - val_acc: 0.8997\n",
      "Epoch 39/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3362 - acc: 0.8604 - val_loss: 0.3061 - val_acc: 0.8652\n",
      "Epoch 40/250\n",
      "1275/1275 [==============================] - 0s 119us/step - loss: 0.3314 - acc: 0.8698 - val_loss: 0.2763 - val_acc: 0.8840\n",
      "Epoch 41/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3282 - acc: 0.8659 - val_loss: 0.3166 - val_acc: 0.8997\n",
      "Epoch 42/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3350 - acc: 0.8620 - val_loss: 0.2747 - val_acc: 0.8934\n",
      "Epoch 43/250\n",
      "1275/1275 [==============================] - 0s 134us/step - loss: 0.3327 - acc: 0.8651 - val_loss: 0.3435 - val_acc: 0.8558\n",
      "Epoch 44/250\n",
      "1275/1275 [==============================] - 0s 145us/step - loss: 0.3280 - acc: 0.8690 - val_loss: 0.2803 - val_acc: 0.8652\n",
      "Epoch 45/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3292 - acc: 0.8729 - val_loss: 0.3576 - val_acc: 0.8903\n",
      "Epoch 46/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3198 - acc: 0.8792 - val_loss: 0.2878 - val_acc: 0.9028\n",
      "Epoch 47/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3173 - acc: 0.8745 - val_loss: 0.3475 - val_acc: 0.8527\n",
      "Epoch 48/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.3370 - acc: 0.8627 - val_loss: 0.2675 - val_acc: 0.8871\n",
      "Epoch 49/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3286 - acc: 0.8635 - val_loss: 0.5002 - val_acc: 0.7774\n",
      "Epoch 50/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3381 - acc: 0.8682 - val_loss: 0.3173 - val_acc: 0.8966\n",
      "Epoch 51/250\n",
      "1275/1275 [==============================] - 0s 117us/step - loss: 0.3344 - acc: 0.8706 - val_loss: 0.3948 - val_acc: 0.8307\n",
      "Epoch 52/250\n",
      "1275/1275 [==============================] - 0s 102us/step - loss: 0.3324 - acc: 0.8729 - val_loss: 0.3588 - val_acc: 0.8401\n",
      "Epoch 53/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3483 - acc: 0.8612 - val_loss: 0.2946 - val_acc: 0.8934\n",
      "Epoch 54/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3186 - acc: 0.8745 - val_loss: 0.2692 - val_acc: 0.8934\n",
      "Epoch 55/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3330 - acc: 0.8604 - val_loss: 0.2936 - val_acc: 0.8809\n",
      "Epoch 56/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3224 - acc: 0.8784 - val_loss: 0.2757 - val_acc: 0.8746\n",
      "Epoch 57/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3101 - acc: 0.8784 - val_loss: 0.3723 - val_acc: 0.8433\n",
      "Epoch 58/250\n",
      "1275/1275 [==============================] - 0s 104us/step - loss: 0.3308 - acc: 0.8706 - val_loss: 0.3442 - val_acc: 0.8652\n",
      "Epoch 59/250\n",
      "1275/1275 [==============================] - 0s 130us/step - loss: 0.3246 - acc: 0.8675 - val_loss: 0.2873 - val_acc: 0.8683\n",
      "Epoch 60/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - ETA: 0s - loss: 0.3195 - acc: 0.871 - 0s 109us/step - loss: 0.3185 - acc: 0.8745 - val_loss: 0.2785 - val_acc: 0.9060\n",
      "Epoch 61/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3242 - acc: 0.8722 - val_loss: 0.3076 - val_acc: 0.9060\n",
      "Epoch 62/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3309 - acc: 0.8722 - val_loss: 0.2613 - val_acc: 0.8871\n",
      "Epoch 63/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3500 - acc: 0.8612 - val_loss: 0.3142 - val_acc: 0.8871\n",
      "Epoch 64/250\n",
      "1275/1275 [==============================] - 0s 104us/step - loss: 0.3309 - acc: 0.8745 - val_loss: 0.2725 - val_acc: 0.8840\n",
      "Epoch 65/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3125 - acc: 0.8698 - val_loss: 0.3279 - val_acc: 0.8683\n",
      "Epoch 66/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3348 - acc: 0.8714 - val_loss: 0.2913 - val_acc: 0.9028\n",
      "Epoch 67/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3239 - acc: 0.8651 - val_loss: 0.3440 - val_acc: 0.8527\n",
      "Epoch 68/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3196 - acc: 0.8737 - val_loss: 0.2649 - val_acc: 0.9028\n",
      "Epoch 69/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3133 - acc: 0.8816 - val_loss: 0.3112 - val_acc: 0.8997\n",
      "Epoch 70/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3202 - acc: 0.8761 - val_loss: 0.3619 - val_acc: 0.8715\n",
      "Epoch 71/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3259 - acc: 0.8588 - val_loss: 0.2949 - val_acc: 0.8809\n",
      "Epoch 72/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3220 - acc: 0.8729 - val_loss: 0.2678 - val_acc: 0.8871\n",
      "Epoch 73/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3204 - acc: 0.8706 - val_loss: 0.3971 - val_acc: 0.8370\n",
      "Epoch 74/250\n",
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.3204 - acc: 0.8729 - val_loss: 0.2713 - val_acc: 0.9060\n",
      "Epoch 75/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3192 - acc: 0.8635 - val_loss: 0.3000 - val_acc: 0.8777\n",
      "Epoch 76/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3175 - acc: 0.8698 - val_loss: 0.3340 - val_acc: 0.8746\n",
      "Epoch 77/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3085 - acc: 0.8753 - val_loss: 0.3584 - val_acc: 0.8652\n",
      "Epoch 78/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3278 - acc: 0.8698 - val_loss: 0.2889 - val_acc: 0.9028\n",
      "Epoch 79/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.3106 - acc: 0.8745 - val_loss: 0.2781 - val_acc: 0.8840\n",
      "Epoch 80/250\n",
      "1275/1275 [==============================] - 0s 165us/step - loss: 0.3042 - acc: 0.8808 - val_loss: 0.2553 - val_acc: 0.8966\n",
      "Epoch 81/250\n",
      "1275/1275 [==============================] - 0s 110us/step - loss: 0.3247 - acc: 0.8682 - val_loss: 0.2830 - val_acc: 0.9122\n",
      "Epoch 82/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3263 - acc: 0.8706 - val_loss: 0.2713 - val_acc: 0.8966\n",
      "Epoch 83/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3269 - acc: 0.8698 - val_loss: 0.2820 - val_acc: 0.8871\n",
      "Epoch 84/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3210 - acc: 0.8690 - val_loss: 0.3397 - val_acc: 0.8840\n",
      "Epoch 85/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3302 - acc: 0.8698 - val_loss: 0.2882 - val_acc: 0.9060\n",
      "Epoch 86/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3306 - acc: 0.8714 - val_loss: 0.2675 - val_acc: 0.8809\n",
      "Epoch 87/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3273 - acc: 0.8706 - val_loss: 0.2924 - val_acc: 0.8840\n",
      "Epoch 88/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3166 - acc: 0.8706 - val_loss: 0.2856 - val_acc: 0.8809\n",
      "Epoch 89/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3066 - acc: 0.8722 - val_loss: 0.2740 - val_acc: 0.8840\n",
      "Epoch 90/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3180 - acc: 0.8706 - val_loss: 0.2744 - val_acc: 0.8871\n",
      "Epoch 91/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3059 - acc: 0.8769 - val_loss: 0.2784 - val_acc: 0.9028\n",
      "Epoch 92/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3197 - acc: 0.8745 - val_loss: 0.3044 - val_acc: 0.8934\n",
      "Epoch 93/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3218 - acc: 0.8698 - val_loss: 0.2780 - val_acc: 0.9060\n",
      "Epoch 94/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3206 - acc: 0.8761 - val_loss: 0.2779 - val_acc: 0.9028\n",
      "Epoch 95/250\n",
      "1275/1275 [==============================] - 0s 83us/step - loss: 0.3261 - acc: 0.8714 - val_loss: 0.3312 - val_acc: 0.8715\n",
      "Epoch 96/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3168 - acc: 0.8675 - val_loss: 0.2736 - val_acc: 0.8903\n",
      "Epoch 97/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3010 - acc: 0.8706 - val_loss: 0.2856 - val_acc: 0.8966\n",
      "Epoch 98/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3166 - acc: 0.8737 - val_loss: 0.4601 - val_acc: 0.8088\n",
      "Epoch 99/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3308 - acc: 0.8643 - val_loss: 0.2599 - val_acc: 0.8966\n",
      "Epoch 100/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3255 - acc: 0.8690 - val_loss: 0.2836 - val_acc: 0.9091\n",
      "Epoch 101/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3176 - acc: 0.8722 - val_loss: 0.3153 - val_acc: 0.8809\n",
      "Epoch 102/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3242 - acc: 0.8753 - val_loss: 0.3263 - val_acc: 0.8777\n",
      "Epoch 103/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3406 - acc: 0.8643 - val_loss: 0.2894 - val_acc: 0.8997\n",
      "Epoch 104/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3036 - acc: 0.8808 - val_loss: 0.3165 - val_acc: 0.8871\n",
      "Epoch 105/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3115 - acc: 0.8737 - val_loss: 0.2804 - val_acc: 0.8652\n",
      "Epoch 106/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3031 - acc: 0.8776 - val_loss: 0.2782 - val_acc: 0.8934\n",
      "Epoch 107/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3061 - acc: 0.8729 - val_loss: 0.3054 - val_acc: 0.8903\n",
      "Epoch 108/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3078 - acc: 0.8792 - val_loss: 0.2594 - val_acc: 0.8966\n",
      "Epoch 109/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3262 - acc: 0.8690 - val_loss: 0.2649 - val_acc: 0.9028\n",
      "Epoch 110/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3091 - acc: 0.8745 - val_loss: 0.2557 - val_acc: 0.8903\n",
      "Epoch 111/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3040 - acc: 0.8824 - val_loss: 0.2792 - val_acc: 0.8903\n",
      "Epoch 112/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3011 - acc: 0.8831 - val_loss: 0.2939 - val_acc: 0.8934\n",
      "Epoch 113/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3201 - acc: 0.8690 - val_loss: 0.2799 - val_acc: 0.9091\n",
      "Epoch 114/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3118 - acc: 0.8761 - val_loss: 0.3020 - val_acc: 0.8715\n",
      "Epoch 115/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3112 - acc: 0.8784 - val_loss: 0.3126 - val_acc: 0.8746\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.200000\n",
      "\n",
      "Train on 1275 samples, validate on 319 samples\n",
      "Epoch 1/250\n",
      "1275/1275 [==============================] - 1s 1ms/step - loss: 0.4559 - acc: 0.8212 - val_loss: 0.3411 - val_acc: 0.8777\n",
      "Epoch 2/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3979 - acc: 0.8486 - val_loss: 0.3992 - val_acc: 0.8433\n",
      "Epoch 3/250\n",
      "1275/1275 [==============================] - 0s 201us/step - loss: 0.4274 - acc: 0.8329 - val_loss: 0.3023 - val_acc: 0.8683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/250\n",
      "1275/1275 [==============================] - 0s 119us/step - loss: 0.3955 - acc: 0.8416 - val_loss: 0.4516 - val_acc: 0.8025\n",
      "Epoch 5/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3581 - acc: 0.8635 - val_loss: 0.3010 - val_acc: 0.8777\n",
      "Epoch 6/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.4052 - acc: 0.8424 - val_loss: 0.3385 - val_acc: 0.8433\n",
      "Epoch 7/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3644 - acc: 0.8549 - val_loss: 0.3444 - val_acc: 0.8401\n",
      "Epoch 8/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3752 - acc: 0.8408 - val_loss: 0.4652 - val_acc: 0.8715\n",
      "Epoch 9/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3970 - acc: 0.8502 - val_loss: 0.2878 - val_acc: 0.8966\n",
      "Epoch 10/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3566 - acc: 0.8698 - val_loss: 0.4853 - val_acc: 0.8150\n",
      "Epoch 11/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3579 - acc: 0.8573 - val_loss: 0.3790 - val_acc: 0.8495\n",
      "Epoch 12/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3782 - acc: 0.8565 - val_loss: 0.2924 - val_acc: 0.8840\n",
      "Epoch 13/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3676 - acc: 0.8604 - val_loss: 0.2959 - val_acc: 0.8840\n",
      "Epoch 14/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3555 - acc: 0.8549 - val_loss: 0.4885 - val_acc: 0.8213\n",
      "Epoch 15/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3940 - acc: 0.8376 - val_loss: 0.6541 - val_acc: 0.6928\n",
      "Epoch 16/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3742 - acc: 0.8518 - val_loss: 0.3120 - val_acc: 0.8809\n",
      "Epoch 17/250\n",
      "1275/1275 [==============================] - 0s 69us/step - loss: 0.3772 - acc: 0.8478 - val_loss: 0.3998 - val_acc: 0.8401\n",
      "Epoch 18/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3504 - acc: 0.8643 - val_loss: 0.3956 - val_acc: 0.8527\n",
      "Epoch 19/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3625 - acc: 0.8502 - val_loss: 0.3036 - val_acc: 0.8746\n",
      "Epoch 20/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3552 - acc: 0.8620 - val_loss: 0.3666 - val_acc: 0.8871\n",
      "Epoch 21/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3519 - acc: 0.8682 - val_loss: 0.4345 - val_acc: 0.8276\n",
      "Epoch 22/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3456 - acc: 0.8643 - val_loss: 0.3428 - val_acc: 0.8840\n",
      "Epoch 23/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3761 - acc: 0.8557 - val_loss: 0.3434 - val_acc: 0.8621\n",
      "Epoch 24/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3575 - acc: 0.8620 - val_loss: 0.3217 - val_acc: 0.8652\n",
      "Epoch 25/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3552 - acc: 0.8612 - val_loss: 0.3495 - val_acc: 0.8433\n",
      "Epoch 26/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3771 - acc: 0.8510 - val_loss: 0.3156 - val_acc: 0.8840\n",
      "Epoch 27/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3540 - acc: 0.8565 - val_loss: 0.3112 - val_acc: 0.8746\n",
      "Epoch 28/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3530 - acc: 0.8651 - val_loss: 0.3687 - val_acc: 0.8339\n",
      "Epoch 29/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3324 - acc: 0.8675 - val_loss: 0.2930 - val_acc: 0.8621\n",
      "Epoch 30/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3440 - acc: 0.8588 - val_loss: 0.3230 - val_acc: 0.8495\n",
      "Epoch 31/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.3734 - acc: 0.8525 - val_loss: 0.3298 - val_acc: 0.8652\n",
      "Epoch 32/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3355 - acc: 0.8651 - val_loss: 0.3035 - val_acc: 0.8621\n",
      "Epoch 33/250\n",
      "1275/1275 [==============================] - 0s 68us/step - loss: 0.3411 - acc: 0.8675 - val_loss: 0.3272 - val_acc: 0.8809\n",
      "Epoch 34/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3650 - acc: 0.8620 - val_loss: 0.2929 - val_acc: 0.9028\n",
      "Epoch 35/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3376 - acc: 0.8659 - val_loss: 0.3213 - val_acc: 0.8746\n",
      "Epoch 36/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3389 - acc: 0.8722 - val_loss: 0.4048 - val_acc: 0.8119\n",
      "Epoch 37/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3603 - acc: 0.8541 - val_loss: 0.3585 - val_acc: 0.8495\n",
      "Epoch 38/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3491 - acc: 0.8690 - val_loss: 0.3837 - val_acc: 0.8715\n",
      "Epoch 39/250\n",
      "1275/1275 [==============================] - 0s 128us/step - loss: 0.3446 - acc: 0.8706 - val_loss: 0.3730 - val_acc: 0.8527\n",
      "Epoch 40/250\n",
      "1275/1275 [==============================] - 0s 115us/step - loss: 0.3365 - acc: 0.8635 - val_loss: 0.5755 - val_acc: 0.7743\n",
      "Epoch 41/250\n",
      "1275/1275 [==============================] - 0s 98us/step - loss: 0.3428 - acc: 0.8604 - val_loss: 0.2625 - val_acc: 0.8871\n",
      "Epoch 42/250\n",
      "1275/1275 [==============================] - 0s 99us/step - loss: 0.3269 - acc: 0.8714 - val_loss: 0.3022 - val_acc: 0.8527\n",
      "Epoch 43/250\n",
      "1275/1275 [==============================] - 0s 95us/step - loss: 0.3569 - acc: 0.8675 - val_loss: 0.3302 - val_acc: 0.8809\n",
      "Epoch 44/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3558 - acc: 0.8596 - val_loss: 0.3315 - val_acc: 0.8589\n",
      "Epoch 45/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3356 - acc: 0.8604 - val_loss: 0.5142 - val_acc: 0.8119\n",
      "Epoch 46/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3410 - acc: 0.8659 - val_loss: 0.3040 - val_acc: 0.8903\n",
      "Epoch 47/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.3613 - acc: 0.8612 - val_loss: 0.2673 - val_acc: 0.8809\n",
      "Epoch 48/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3312 - acc: 0.8706 - val_loss: 0.3393 - val_acc: 0.8558\n",
      "Epoch 49/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3558 - acc: 0.8620 - val_loss: 0.3522 - val_acc: 0.8871\n",
      "Epoch 50/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3495 - acc: 0.8667 - val_loss: 0.3005 - val_acc: 0.8903\n",
      "Epoch 51/250\n",
      "1275/1275 [==============================] - 0s 105us/step - loss: 0.3348 - acc: 0.8635 - val_loss: 0.2950 - val_acc: 0.8809\n",
      "Epoch 52/250\n",
      "1275/1275 [==============================] - 0s 93us/step - loss: 0.3249 - acc: 0.8706 - val_loss: 0.3081 - val_acc: 0.8777\n",
      "Epoch 53/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3441 - acc: 0.8612 - val_loss: 0.2852 - val_acc: 0.8777\n",
      "Epoch 54/250\n",
      "1275/1275 [==============================] - 0s 93us/step - loss: 0.3175 - acc: 0.8816 - val_loss: 0.2836 - val_acc: 0.8903\n",
      "Epoch 55/250\n",
      "1275/1275 [==============================] - 0s 116us/step - loss: 0.3337 - acc: 0.8627 - val_loss: 0.2726 - val_acc: 0.8777\n",
      "Epoch 56/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3344 - acc: 0.8667 - val_loss: 0.3124 - val_acc: 0.8934\n",
      "Epoch 57/250\n",
      "1275/1275 [==============================] - 0s 136us/step - loss: 0.3230 - acc: 0.8675 - val_loss: 0.2528 - val_acc: 0.8903\n",
      "Epoch 58/250\n",
      "1275/1275 [==============================] - 0s 99us/step - loss: 0.3480 - acc: 0.8604 - val_loss: 0.2575 - val_acc: 0.8903\n",
      "Epoch 59/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3496 - acc: 0.8659 - val_loss: 0.2883 - val_acc: 0.8809\n",
      "Epoch 60/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3277 - acc: 0.8659 - val_loss: 0.2746 - val_acc: 0.8934\n",
      "Epoch 61/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3583 - acc: 0.8612 - val_loss: 0.3230 - val_acc: 0.8715\n",
      "Epoch 62/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3385 - acc: 0.8682 - val_loss: 0.2864 - val_acc: 0.8809\n",
      "Epoch 63/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3320 - acc: 0.8635 - val_loss: 0.3418 - val_acc: 0.8715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3654 - acc: 0.8525 - val_loss: 0.3468 - val_acc: 0.8558\n",
      "Epoch 65/250\n",
      "1275/1275 [==============================] - 0s 96us/step - loss: 0.3169 - acc: 0.8792 - val_loss: 0.3876 - val_acc: 0.8589\n",
      "Epoch 66/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3479 - acc: 0.8682 - val_loss: 0.2773 - val_acc: 0.8903\n",
      "Epoch 67/250\n",
      "1275/1275 [==============================] - 0s 97us/step - loss: 0.3204 - acc: 0.8745 - val_loss: 0.4708 - val_acc: 0.8433\n",
      "Epoch 68/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3251 - acc: 0.8690 - val_loss: 0.2781 - val_acc: 0.8840\n",
      "Epoch 69/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3404 - acc: 0.8573 - val_loss: 0.2882 - val_acc: 0.8840\n",
      "Epoch 70/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3345 - acc: 0.8643 - val_loss: 0.2920 - val_acc: 0.8871\n",
      "Epoch 71/250\n",
      "1275/1275 [==============================] - 0s 106us/step - loss: 0.3347 - acc: 0.8690 - val_loss: 0.4736 - val_acc: 0.8339\n",
      "Epoch 72/250\n",
      "1275/1275 [==============================] - 0s 127us/step - loss: 0.3149 - acc: 0.8800 - val_loss: 0.2855 - val_acc: 0.8777\n",
      "Epoch 73/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3428 - acc: 0.8651 - val_loss: 0.2573 - val_acc: 0.9060\n",
      "Epoch 74/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3493 - acc: 0.8620 - val_loss: 0.3472 - val_acc: 0.8652\n",
      "Epoch 75/250\n",
      "1275/1275 [==============================] - 0s 121us/step - loss: 0.3429 - acc: 0.8651 - val_loss: 0.4359 - val_acc: 0.8213\n",
      "Epoch 76/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3449 - acc: 0.8659 - val_loss: 0.3104 - val_acc: 0.8715\n",
      "Epoch 77/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3289 - acc: 0.8706 - val_loss: 0.2723 - val_acc: 0.9060\n",
      "Epoch 78/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3198 - acc: 0.8753 - val_loss: 0.2801 - val_acc: 0.8871\n",
      "Epoch 79/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3386 - acc: 0.8604 - val_loss: 0.3047 - val_acc: 0.8934\n",
      "Epoch 80/250\n",
      "1275/1275 [==============================] - 0s 105us/step - loss: 0.3441 - acc: 0.8612 - val_loss: 0.3482 - val_acc: 0.8589\n",
      "Epoch 81/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3435 - acc: 0.8714 - val_loss: 0.4474 - val_acc: 0.8056\n",
      "Epoch 82/250\n",
      "1275/1275 [==============================] - 0s 109us/step - loss: 0.3306 - acc: 0.8714 - val_loss: 0.2730 - val_acc: 0.8997\n",
      "Epoch 83/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3250 - acc: 0.8620 - val_loss: 0.2957 - val_acc: 0.8966\n",
      "Epoch 84/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3172 - acc: 0.8675 - val_loss: 0.2646 - val_acc: 0.8934\n",
      "Epoch 85/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3113 - acc: 0.8808 - val_loss: 0.3312 - val_acc: 0.8652\n",
      "Epoch 86/250\n",
      "1275/1275 [==============================] - 0s 99us/step - loss: 0.3414 - acc: 0.8557 - val_loss: 0.2603 - val_acc: 0.8903\n",
      "Epoch 87/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3270 - acc: 0.8729 - val_loss: 0.2646 - val_acc: 0.8997\n",
      "Epoch 88/250\n",
      "1275/1275 [==============================] - 0s 134us/step - loss: 0.3015 - acc: 0.8808 - val_loss: 0.2712 - val_acc: 0.8871\n",
      "Epoch 89/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3194 - acc: 0.8776 - val_loss: 0.2708 - val_acc: 0.9060\n",
      "Epoch 90/250\n",
      "1275/1275 [==============================] - 0s 122us/step - loss: 0.3299 - acc: 0.8651 - val_loss: 0.2422 - val_acc: 0.9091\n",
      "Epoch 91/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3199 - acc: 0.8706 - val_loss: 0.3134 - val_acc: 0.8558\n",
      "Epoch 92/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3281 - acc: 0.8635 - val_loss: 0.3121 - val_acc: 0.8746\n",
      "Epoch 93/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3263 - acc: 0.8745 - val_loss: 0.5250 - val_acc: 0.7806\n",
      "Epoch 94/250\n",
      "1275/1275 [==============================] - 0s 119us/step - loss: 0.3428 - acc: 0.8667 - val_loss: 0.2745 - val_acc: 0.8903\n",
      "Epoch 95/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3164 - acc: 0.8737 - val_loss: 0.2594 - val_acc: 0.8903\n",
      "Epoch 96/250\n",
      "1275/1275 [==============================] - 0s 98us/step - loss: 0.3196 - acc: 0.8761 - val_loss: 0.2485 - val_acc: 0.9122\n",
      "Epoch 97/250\n",
      "1275/1275 [==============================] - 0s 112us/step - loss: 0.3360 - acc: 0.8533 - val_loss: 0.2479 - val_acc: 0.9091\n",
      "Epoch 98/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3267 - acc: 0.8776 - val_loss: 0.3773 - val_acc: 0.8527\n",
      "Epoch 99/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3083 - acc: 0.8737 - val_loss: 0.2686 - val_acc: 0.8809\n",
      "Epoch 100/250\n",
      "1275/1275 [==============================] - 0s 81us/step - loss: 0.3230 - acc: 0.8682 - val_loss: 0.3075 - val_acc: 0.8715\n",
      "Epoch 101/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3485 - acc: 0.8580 - val_loss: 0.3126 - val_acc: 0.8934\n",
      "Epoch 102/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3282 - acc: 0.8659 - val_loss: 0.2660 - val_acc: 0.9028\n",
      "Epoch 103/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3169 - acc: 0.8714 - val_loss: 0.3535 - val_acc: 0.8401\n",
      "Epoch 104/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3103 - acc: 0.8722 - val_loss: 0.2470 - val_acc: 0.9028\n",
      "Epoch 105/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3284 - acc: 0.8698 - val_loss: 0.3482 - val_acc: 0.8495\n",
      "Epoch 106/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3284 - acc: 0.8588 - val_loss: 0.3401 - val_acc: 0.8934\n",
      "Epoch 107/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3037 - acc: 0.8722 - val_loss: 0.2832 - val_acc: 0.8997\n",
      "Epoch 108/250\n",
      "1275/1275 [==============================] - 0s 112us/step - loss: 0.3274 - acc: 0.8651 - val_loss: 0.2707 - val_acc: 0.8903\n",
      "Epoch 109/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3041 - acc: 0.8824 - val_loss: 0.3191 - val_acc: 0.8777\n",
      "Epoch 110/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3391 - acc: 0.8722 - val_loss: 0.2662 - val_acc: 0.9060\n",
      "Epoch 111/250\n",
      "1275/1275 [==============================] - 0s 102us/step - loss: 0.3166 - acc: 0.8769 - val_loss: 0.2636 - val_acc: 0.9060\n",
      "Epoch 112/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3272 - acc: 0.8714 - val_loss: 0.3114 - val_acc: 0.8840\n",
      "Epoch 113/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3127 - acc: 0.8722 - val_loss: 0.2505 - val_acc: 0.9028\n",
      "Epoch 114/250\n",
      "1275/1275 [==============================] - 0s 70us/step - loss: 0.3207 - acc: 0.8682 - val_loss: 0.3204 - val_acc: 0.8683\n",
      "Epoch 115/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3077 - acc: 0.8800 - val_loss: 0.2698 - val_acc: 0.9091\n",
      "Epoch 116/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3153 - acc: 0.8784 - val_loss: 0.2429 - val_acc: 0.9091\n",
      "Epoch 117/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3130 - acc: 0.8737 - val_loss: 0.3474 - val_acc: 0.8495\n",
      "Epoch 118/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3336 - acc: 0.8667 - val_loss: 0.2719 - val_acc: 0.8934\n",
      "Epoch 119/250\n",
      "1275/1275 [==============================] - 0s 141us/step - loss: 0.3111 - acc: 0.8714 - val_loss: 0.2811 - val_acc: 0.8934\n",
      "Epoch 120/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3009 - acc: 0.8784 - val_loss: 0.2535 - val_acc: 0.9028\n",
      "Epoch 121/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3330 - acc: 0.8651 - val_loss: 0.2907 - val_acc: 0.8652\n",
      "Epoch 122/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3200 - acc: 0.8635 - val_loss: 0.2634 - val_acc: 0.8966\n",
      "Epoch 123/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3169 - acc: 0.8714 - val_loss: 0.2757 - val_acc: 0.8934\n",
      "Epoch 124/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3218 - acc: 0.8706 - val_loss: 0.3936 - val_acc: 0.8401\n",
      "Epoch 125/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3139 - acc: 0.8698 - val_loss: 0.2876 - val_acc: 0.8903\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.300000\n",
      "\n",
      "Train on 1275 samples, validate on 319 samples\n",
      "Epoch 1/250\n",
      "1275/1275 [==============================] - 2s 1ms/step - loss: 0.3758 - acc: 0.8369 - val_loss: 0.3271 - val_acc: 0.8840\n",
      "Epoch 2/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3554 - acc: 0.8620 - val_loss: 0.2536 - val_acc: 0.9060\n",
      "Epoch 3/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3663 - acc: 0.8604 - val_loss: 0.2573 - val_acc: 0.9028\n",
      "Epoch 4/250\n",
      "1275/1275 [==============================] - 0s 88us/step - loss: 0.3327 - acc: 0.8667 - val_loss: 0.2763 - val_acc: 0.8903\n",
      "Epoch 5/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3422 - acc: 0.8596 - val_loss: 0.2831 - val_acc: 0.8840\n",
      "Epoch 6/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3526 - acc: 0.8557 - val_loss: 0.3562 - val_acc: 0.8401\n",
      "Epoch 7/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3466 - acc: 0.8580 - val_loss: 0.3629 - val_acc: 0.8934\n",
      "Epoch 8/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3595 - acc: 0.8494 - val_loss: 0.3982 - val_acc: 0.8746\n",
      "Epoch 9/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3603 - acc: 0.8620 - val_loss: 0.2966 - val_acc: 0.8840\n",
      "Epoch 10/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3378 - acc: 0.8643 - val_loss: 0.2774 - val_acc: 0.8966\n",
      "Epoch 11/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3591 - acc: 0.8510 - val_loss: 0.3154 - val_acc: 0.8809\n",
      "Epoch 12/250\n",
      "1275/1275 [==============================] - 0s 125us/step - loss: 0.3299 - acc: 0.8706 - val_loss: 0.2530 - val_acc: 0.9091\n",
      "Epoch 13/250\n",
      "1275/1275 [==============================] - 0s 115us/step - loss: 0.3910 - acc: 0.8424 - val_loss: 0.5321 - val_acc: 0.7555\n",
      "Epoch 14/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.4752 - acc: 0.8282 - val_loss: 0.3012 - val_acc: 0.8997\n",
      "Epoch 15/250\n",
      "1275/1275 [==============================] - 0s 106us/step - loss: 0.3560 - acc: 0.8714 - val_loss: 0.2921 - val_acc: 0.8871\n",
      "Epoch 16/250\n",
      "1275/1275 [==============================] - 0s 110us/step - loss: 0.3509 - acc: 0.8596 - val_loss: 0.2766 - val_acc: 0.8777\n",
      "Epoch 17/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3457 - acc: 0.8612 - val_loss: 0.2957 - val_acc: 0.8997\n",
      "Epoch 18/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3500 - acc: 0.8565 - val_loss: 0.2906 - val_acc: 0.8966\n",
      "Epoch 19/250\n",
      "1275/1275 [==============================] - 0s 140us/step - loss: 0.3521 - acc: 0.8635 - val_loss: 0.4723 - val_acc: 0.8715\n",
      "Epoch 20/250\n",
      "1275/1275 [==============================] - 0s 118us/step - loss: 0.3396 - acc: 0.8659 - val_loss: 0.3057 - val_acc: 0.8777\n",
      "Epoch 21/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3318 - acc: 0.8675 - val_loss: 0.3224 - val_acc: 0.8621\n",
      "Epoch 22/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3432 - acc: 0.8627 - val_loss: 0.2919 - val_acc: 0.8934\n",
      "Epoch 23/250\n",
      "1275/1275 [==============================] - 0s 106us/step - loss: 0.3718 - acc: 0.8494 - val_loss: 0.2831 - val_acc: 0.8997\n",
      "Epoch 24/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3532 - acc: 0.8627 - val_loss: 0.4409 - val_acc: 0.8464\n",
      "Epoch 25/250\n",
      "1275/1275 [==============================] - 0s 73us/step - loss: 0.3532 - acc: 0.8588 - val_loss: 0.2946 - val_acc: 0.8966\n",
      "Epoch 26/250\n",
      "1275/1275 [==============================] - 0s 110us/step - loss: 0.3410 - acc: 0.8706 - val_loss: 0.3592 - val_acc: 0.8777\n",
      "Epoch 27/250\n",
      "1275/1275 [==============================] - 0s 110us/step - loss: 0.3442 - acc: 0.8596 - val_loss: 0.3514 - val_acc: 0.8777\n",
      "Epoch 28/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3046 - acc: 0.8722 - val_loss: 0.2742 - val_acc: 0.9028\n",
      "Epoch 29/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3225 - acc: 0.8706 - val_loss: 0.2628 - val_acc: 0.8871\n",
      "Epoch 30/250\n",
      "1275/1275 [==============================] - 0s 106us/step - loss: 0.3232 - acc: 0.8722 - val_loss: 0.2869 - val_acc: 0.9028\n",
      "Epoch 31/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3410 - acc: 0.8698 - val_loss: 0.2880 - val_acc: 0.8840\n",
      "Epoch 32/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3517 - acc: 0.8596 - val_loss: 0.2815 - val_acc: 0.8840\n",
      "Epoch 33/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3253 - acc: 0.8792 - val_loss: 0.2519 - val_acc: 0.9091\n",
      "Epoch 34/250\n",
      "1275/1275 [==============================] - 0s 101us/step - loss: 0.3247 - acc: 0.8706 - val_loss: 0.2715 - val_acc: 0.8997\n",
      "Epoch 35/250\n",
      "1275/1275 [==============================] - 0s 98us/step - loss: 0.3201 - acc: 0.8745 - val_loss: 0.2843 - val_acc: 0.8966\n",
      "Epoch 36/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3270 - acc: 0.8612 - val_loss: 0.2656 - val_acc: 0.8997\n",
      "Epoch 37/250\n",
      "1275/1275 [==============================] - 0s 106us/step - loss: 0.3119 - acc: 0.8769 - val_loss: 0.3144 - val_acc: 0.8777\n",
      "Epoch 38/250\n",
      "1275/1275 [==============================] - 0s 94us/step - loss: 0.3260 - acc: 0.8690 - val_loss: 0.3169 - val_acc: 0.8934\n",
      "Epoch 39/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3328 - acc: 0.8690 - val_loss: 0.3128 - val_acc: 0.8934\n",
      "Epoch 40/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3180 - acc: 0.8698 - val_loss: 0.2937 - val_acc: 0.8934\n",
      "Epoch 41/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3217 - acc: 0.8698 - val_loss: 0.2518 - val_acc: 0.8966\n",
      "Epoch 42/250\n",
      "1275/1275 [==============================] - 0s 100us/step - loss: 0.3324 - acc: 0.8651 - val_loss: 0.2598 - val_acc: 0.8840\n",
      "Epoch 43/250\n",
      "1275/1275 [==============================] - 0s 99us/step - loss: 0.3244 - acc: 0.8659 - val_loss: 0.2677 - val_acc: 0.9185\n",
      "Epoch 44/250\n",
      "1275/1275 [==============================] - 0s 127us/step - loss: 0.3090 - acc: 0.8753 - val_loss: 0.2554 - val_acc: 0.9091\n",
      "Epoch 45/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.3249 - acc: 0.8620 - val_loss: 0.2621 - val_acc: 0.9060\n",
      "Epoch 46/250\n",
      "1275/1275 [==============================] - 0s 103us/step - loss: 0.3437 - acc: 0.8667 - val_loss: 0.3129 - val_acc: 0.8683\n",
      "Epoch 47/250\n",
      "1275/1275 [==============================] - 0s 91us/step - loss: 0.3124 - acc: 0.8698 - val_loss: 0.2744 - val_acc: 0.8871\n",
      "Epoch 48/250\n",
      "1275/1275 [==============================] - 0s 127us/step - loss: 0.3238 - acc: 0.8714 - val_loss: 0.2662 - val_acc: 0.8997\n",
      "Epoch 49/250\n",
      "1275/1275 [==============================] - 0s 95us/step - loss: 0.3431 - acc: 0.8737 - val_loss: 0.3700 - val_acc: 0.8997\n",
      "Epoch 50/250\n",
      "1275/1275 [==============================] - 0s 82us/step - loss: 0.3207 - acc: 0.8753 - val_loss: 0.2561 - val_acc: 0.9060\n",
      "Epoch 51/250\n",
      "1275/1275 [==============================] - 0s 90us/step - loss: 0.3337 - acc: 0.8659 - val_loss: 0.2986 - val_acc: 0.8746\n",
      "Epoch 52/250\n",
      "1275/1275 [==============================] - 0s 154us/step - loss: 0.3405 - acc: 0.8659 - val_loss: 0.5218 - val_acc: 0.7868\n",
      "Epoch 53/250\n",
      "1275/1275 [==============================] - 0s 123us/step - loss: 0.3265 - acc: 0.8620 - val_loss: 0.2902 - val_acc: 0.8809\n",
      "Epoch 54/250\n",
      "1275/1275 [==============================] - 0s 77us/step - loss: 0.3160 - acc: 0.8745 - val_loss: 0.2421 - val_acc: 0.9060\n",
      "Epoch 55/250\n",
      "1275/1275 [==============================] - 0s 112us/step - loss: 0.3191 - acc: 0.8729 - val_loss: 0.3265 - val_acc: 0.8903\n",
      "Epoch 56/250\n",
      "1275/1275 [==============================] - 0s 114us/step - loss: 0.4262 - acc: 0.8478 - val_loss: 0.3031 - val_acc: 0.8809\n",
      "Epoch 57/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 0s 87us/step - loss: 0.3477 - acc: 0.8604 - val_loss: 0.2862 - val_acc: 0.8840\n",
      "Epoch 58/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3422 - acc: 0.8722 - val_loss: 0.4251 - val_acc: 0.8558\n",
      "Epoch 59/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3302 - acc: 0.8651 - val_loss: 0.3157 - val_acc: 0.8840\n",
      "Epoch 60/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3275 - acc: 0.8753 - val_loss: 0.2760 - val_acc: 0.9028\n",
      "Epoch 61/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3316 - acc: 0.8643 - val_loss: 0.2539 - val_acc: 0.9060\n",
      "Epoch 62/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3382 - acc: 0.8745 - val_loss: 0.3022 - val_acc: 0.9060\n",
      "Epoch 63/250\n",
      "1275/1275 [==============================] - 0s 72us/step - loss: 0.3226 - acc: 0.8682 - val_loss: 0.2584 - val_acc: 0.8871\n",
      "Epoch 64/250\n",
      "1275/1275 [==============================] - 0s 102us/step - loss: 0.3143 - acc: 0.8682 - val_loss: 0.3335 - val_acc: 0.8589\n",
      "Epoch 65/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3303 - acc: 0.8675 - val_loss: 0.3202 - val_acc: 0.8809\n",
      "Epoch 66/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3108 - acc: 0.8776 - val_loss: 0.2627 - val_acc: 0.8966\n",
      "Epoch 67/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3303 - acc: 0.8580 - val_loss: 0.2526 - val_acc: 0.8934\n",
      "Epoch 68/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3020 - acc: 0.8847 - val_loss: 0.3010 - val_acc: 0.8777\n",
      "Epoch 69/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3246 - acc: 0.8776 - val_loss: 0.2778 - val_acc: 0.9185\n",
      "Epoch 70/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3195 - acc: 0.8831 - val_loss: 0.3040 - val_acc: 0.8966\n",
      "Epoch 71/250\n",
      "1275/1275 [==============================] - 0s 70us/step - loss: 0.3073 - acc: 0.8831 - val_loss: 0.2610 - val_acc: 0.8903\n",
      "Epoch 72/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3144 - acc: 0.8698 - val_loss: 0.3320 - val_acc: 0.8558\n",
      "Epoch 73/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3165 - acc: 0.8722 - val_loss: 0.2879 - val_acc: 0.8683\n",
      "Epoch 74/250\n",
      "1275/1275 [==============================] - 0s 78us/step - loss: 0.3428 - acc: 0.8588 - val_loss: 0.3087 - val_acc: 0.8746\n",
      "Epoch 75/250\n",
      "1275/1275 [==============================] - 0s 71us/step - loss: 0.3377 - acc: 0.8706 - val_loss: 0.3207 - val_acc: 0.8840\n",
      "Epoch 76/250\n",
      "1275/1275 [==============================] - 0s 92us/step - loss: 0.3223 - acc: 0.8729 - val_loss: 0.2684 - val_acc: 0.9185\n",
      "Epoch 77/250\n",
      "1275/1275 [==============================] - 0s 107us/step - loss: 0.3134 - acc: 0.8682 - val_loss: 0.3401 - val_acc: 0.8527\n",
      "Epoch 78/250\n",
      "1275/1275 [==============================] - 0s 84us/step - loss: 0.3140 - acc: 0.8706 - val_loss: 0.2451 - val_acc: 0.8966\n",
      "Epoch 79/250\n",
      "1275/1275 [==============================] - 0s 80us/step - loss: 0.3127 - acc: 0.8737 - val_loss: 0.2765 - val_acc: 0.8903\n",
      "Epoch 80/250\n",
      "1275/1275 [==============================] - 0s 74us/step - loss: 0.3121 - acc: 0.8769 - val_loss: 0.2709 - val_acc: 0.8903\n",
      "Epoch 81/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3127 - acc: 0.8737 - val_loss: 0.3161 - val_acc: 0.8589\n",
      "Epoch 82/250\n",
      "1275/1275 [==============================] - 0s 76us/step - loss: 0.3063 - acc: 0.8808 - val_loss: 0.2838 - val_acc: 0.8966\n",
      "Epoch 83/250\n",
      "1275/1275 [==============================] - 0s 89us/step - loss: 0.3682 - acc: 0.8510 - val_loss: 0.3391 - val_acc: 0.8401\n",
      "Epoch 84/250\n",
      "1275/1275 [==============================] - 0s 86us/step - loss: 0.3062 - acc: 0.8761 - val_loss: 0.2929 - val_acc: 0.8809\n",
      "Epoch 85/250\n",
      "1275/1275 [==============================] - 0s 75us/step - loss: 0.3011 - acc: 0.8769 - val_loss: 0.2758 - val_acc: 0.8966\n",
      "Epoch 86/250\n",
      "1275/1275 [==============================] - 0s 85us/step - loss: 0.3438 - acc: 0.8596 - val_loss: 0.2737 - val_acc: 0.9122\n",
      "Epoch 87/250\n",
      "1275/1275 [==============================] - ETA: 0s - loss: 0.3217 - acc: 0.871 - 0s 111us/step - loss: 0.3172 - acc: 0.8729 - val_loss: 0.3353 - val_acc: 0.8589\n",
      "Epoch 88/250\n",
      "1275/1275 [==============================] - 0s 111us/step - loss: 0.3161 - acc: 0.8706 - val_loss: 0.2721 - val_acc: 0.8903\n",
      "Epoch 89/250\n",
      "1275/1275 [==============================] - 0s 79us/step - loss: 0.3317 - acc: 0.8690 - val_loss: 0.2799 - val_acc: 0.9122\n"
     ]
    }
   ],
   "source": [
    "# compilling and fitting the model\n",
    "# we will use Stochastic gradient descent optimizer with .01, 0.03, 0.05, 0.07, 0.2, 0.3 learning rate.\n",
    "# we will create a eanly stopping function with patience 35 if our epoch doesn't show any learning behaviour.\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience= 35)\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "\n",
    "lr_to_test = [.01, 0.03, 0.05, 0.07, 0.2, 0.3]\n",
    "\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    \n",
    "    my_optimizer = SGD(lr= lr)\n",
    "    \n",
    "    # compilling model\n",
    "    \n",
    "    model.compile(optimizer= my_optimizer, loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "    \n",
    "    # fitting model\n",
    "    \n",
    "    history= model.fit(x_train, y_train, validation_split= .2, epochs= 250, callbacks= [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[0. 0. 1.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=2\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=2\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=1\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=1\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=1\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[0. 1. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[0. 0. 1.], Predicted=2\n",
      "X=[1. 0. 0.], Predicted=0\n",
      "X=[1. 0. 0.], Predicted=1\n"
     ]
    }
   ],
   "source": [
    "# predicting classes\n",
    "\n",
    "prediction_class = model.predict_classes(x_test)\n",
    "\n",
    "for i in range(len(prediction)):\n",
    "    print(\"X=%s, Predicted=%s\" % (y_test[i], prediction_class[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92       420\n",
      "           1       0.66      0.48      0.55        79\n",
      "           2       0.90      0.79      0.84        33\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       532\n",
      "   macro avg       0.82      0.74      0.77       532\n",
      "weighted avg       0.86      0.87      0.86       532\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "\n",
      "\n",
      "[[400  17   3]\n",
      " [ 41  38   0]\n",
      " [  4   3  26]]\n",
      "\n",
      "\n",
      "Overall Accuracy\n",
      "\n",
      "\n",
      "0.8721804511278195\n"
     ]
    }
   ],
   "source": [
    "# creating all the metrics result\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print('Classification Report')\n",
    "print('\\n')\n",
    "print(classification_report(np.argmax(y_test,axis=1), prediction_class))\n",
    "print('\\n')\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print('\\n')\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), prediction_class))\n",
    "print('\\n')\n",
    "\n",
    "print('Overall Accuracy')\n",
    "print('\\n')\n",
    "print(accuracy_score(np.argmax(y_test,axis=1), prediction_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# last but not the least plotting the model performance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
